{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f926108",
   "metadata": {},
   "source": [
    "# TensorFlow Developer Professional Exam Sourcebook\n",
    "\n",
    "The following notebook is a compilation of notes about different terms, topics, processes, and techniques related to the skills and knowledge that are tested on in the TensorFlow Developer Exam. It will contain plain text explanation, pseudo-code, and code snippets throughout depending on the context.\n",
    "\n",
    "The contents are roughly as follows:\n",
    "\n",
    "***Processes***\n",
    "\n",
    "*(1) Basic Neural Net Regression*\n",
    "\n",
    "*(2) Basic Neural Net Classification*\n",
    "\n",
    "*(3) Convolutional Neural Networks*\n",
    "\n",
    "*(4) Transfer Learning - Feature Extraction*\n",
    "\n",
    "*(5) Transfer Learning - Fine-Tuning*\n",
    "\n",
    "*(6) Natural Language Processing*\n",
    "\n",
    "*(7) Time Series - Compiled Process (from 2 examples)*\n",
    "\n",
    "***Non-Problem-Specific Tasks***\n",
    "- Save models in various formats\n",
    "- Plotting loss & accuracy\n",
    "- Preventing overfitting\n",
    "- Data augmentation\n",
    "- Dropout\n",
    "- Batch loading of data\n",
    "- Callbacks\n",
    "- Dataset formats (JSON, CSV, etc.)\n",
    "- Dynamically adjusting learning rates\n",
    "- Visualization of various clf + other metrics\n",
    "- Making a dataset performant using tf.data API\n",
    "- Creating ensemble models\n",
    "- Accounting for horizon variance in time series prediction\n",
    "- Plotting prediction intervals\n",
    "\n",
    "\n",
    "***General Vocabulary and Terms List for Flashcard Study***\n",
    "\n",
    "Vocabulary terms can be found in bold throughout the notebook and are listed at the end of the notebook in alphabetical order with a brief definition and/or description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8accae",
   "metadata": {},
   "source": [
    "# Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6209f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries for occasional use throughout notebook\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "# Common sklearn imports\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbbe11",
   "metadata": {},
   "source": [
    "## *(1) Basic Neural Network Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b4cb8",
   "metadata": {},
   "source": [
    "#### *Create/collect feature and labels*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e98a2",
   "metadata": {},
   "source": [
    "This step involves defining the input features and the corresponding output labels for your regression problem. You should have a clear understanding of the problem you're trying to solve, and select the appropriate **features** and **labels** for your dataset. This step may also involve pre-processing and cleaning the data to **remove outliers**, **missing values**, or other anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af932d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snippets you may find useful\n",
    "ins = pd.read_csv(\"data/insurance_data/insurance.csv\")\n",
    "\n",
    "ins_copy = ins.copy() # create copy for split\n",
    "\n",
    "# Check for missing values/duplicates (important to impute before splitting)\n",
    "ins.isna().sum()\n",
    "ins.duplicated().sum()\n",
    "\n",
    "# Create features and labels\n",
    "X = ins_copy.drop(\"charges\", axis=1)\n",
    "y = ins_copy[\"charges\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629167b",
   "metadata": {},
   "source": [
    "#### *Explore and visualize the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80a25a",
   "metadata": {},
   "source": [
    "Once you have collected the data, it's important to **explore** and **visualize** it to gain insights and identify patterns. You should use techniques such as **scatter plots**, **histograms**, and **correlation matrices** to understand the relationships between the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore details of the specific dataset\n",
    "ins.dtypes # check column data types\n",
    "ins.shape\n",
    "ins.info()\n",
    "ins.describe()\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the dataset to maximize familiarity\n",
    "ins.head() # pass no. of columns you want to see\n",
    "ins.tail()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10)) # plot hist chart\n",
    "plt.title(\"`Charge` amount distribution by `sex`\")\n",
    "plt.xlim(ins.charges.min(), ins.charges.max())\n",
    "sns.histplot(data=ins,\n",
    "              x=\"charges\",\n",
    "              hue=\"sex\",\n",
    "             palette=\"plasma\", kde=True);\n",
    "\n",
    "plt.figure(figsize=(12,10)) # alterative hist\n",
    "plt.xlim(18, 64)\n",
    "sns.histplot(ins[\"age\"], bins=10, kde=True, alpha=0.6);\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10)) # plot strip plot\n",
    "plt.title(\"Age-Charge Correlation\")\n",
    "plt.xticks(rotation=60)\n",
    "sns.stripplot(x=\"age\",\n",
    "              y=\"charges\",\n",
    "             data=ins,\n",
    "             hue=\"sex\",\n",
    "             palette=\"plasma\",\n",
    "             dodge=False);\n",
    "\n",
    "\n",
    "unique_regions = ins.region.unique() # find unique categorical values, plot\n",
    "\n",
    "plt.style.use('ggplot') # choosing sns style\n",
    "\n",
    "by_reg = pd.pivot_table(ins, index=\"region\", values=\"charges\")\n",
    "by_reg = by_reg.sort_values(by=\"charges\", ascending=True)\n",
    "by_reg\n",
    "\n",
    "plot = by_reg.plot(kind=\"barh\", figsize=(12,8),\n",
    "                   title=\"Charges by Region\", legend=True, \n",
    "                   edgecolor=\"black\", lw=2)\n",
    "\n",
    "\n",
    "sex = pd.pivot_table(ins, index=\"sex\", values=\"charges\") # check overall %s\n",
    "sex.plot.pie(figsize=(5,5), subplots=True,\n",
    "             autopct='%1.1f%%', title=\"Charges by sex\");\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10)) # searching for correlations w/ more hist plots\n",
    "plt.title(\"`Charges` amount distribution by `smoker`\")\n",
    "plt.xlim(ins.charges.min(), ins.charges.max())\n",
    "sns.histplot(data=ins,\n",
    "              x=\"charges\",\n",
    "              hue=\"smoker\",\n",
    "             palette=\"magma\", kde=True);\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "by_children = pd.pivot_table(ins, index=\"children\", values=\"charges\")\n",
    "by_children = by_children.sort_values(by=\"children\", ascending=True)\n",
    "by_children\n",
    "\n",
    "plot = by_children.plot(kind=\"barh\", figsize=(12,8), # plotting barh \n",
    "                   title=\"Charges by number of Children\", legend=True, \n",
    "                   edgecolor=\"black\", lw=2)\n",
    "\n",
    "                    # pivot table\n",
    "bmi = pd.pivot_table(ins, index=\"age\", values=\"bmi\", aggfunc=\"mean\")\n",
    "bmi.plot(kind=\"line\", figsize=(12,8), lw=4, marker=\"o\",\n",
    "         markersize=7, markerfacecolor=\"white\",\n",
    "         markeredgecolor=\"black\", markeredgewidth=2)\n",
    "plt.title(\"Average BMI by Age\")\n",
    "plt.xlim(15, 70)\n",
    "plt.ylim(28, 33)\n",
    "plt.axhline(y=ins.bmi.mean(), ls=\"--\", color=\"black\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5)) # plot numerical correlation heatmap\n",
    "sns.heatmap(cp_ins.corr(), annot=True, cmap=\"plasma\");\n",
    "\n",
    "\n",
    "cp_ins[\"sex\"]=pd.factorize(cp_ins.sex)[0] # categorical --> numerical\n",
    "cp_ins[\"smoker\"]=pd.factorize(cp_ins.smoker)[0]\n",
    "cp_ins[\"region\"]=pd.factorize(cp_ins.region)[0]\n",
    "\n",
    "plt.figure(figsize=(10,8)) # another correlation matrix/heatmap\n",
    "sns.heatmap(cp_ins.corr(), annot=True, cmap=\"icefire\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9570fe",
   "metadata": {},
   "source": [
    "#### *Check shapes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27a489",
   "metadata": {},
   "source": [
    "Before building the neural network, you should ensure that the dimensions of the input and output data are consistent with the expected shapes of the model. This involves checking the shape of the input features, the number of output labels, and the number of samples in the training/validation/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf07296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes of data\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test) # find lengths (samples)\n",
    "\n",
    "\n",
    "\n",
    "# For other types of info in TensorFlow...\n",
    "example = tf.constant([1, 2, 3])\n",
    "\n",
    "example.shape\n",
    "example.dtype\n",
    "example.ndim\n",
    "example[x] # index at some location 'x'\n",
    "tf.size(example)\n",
    "tf.cast(example, dtype=tf.DTYPE_HERE)\n",
    "\n",
    "tf.reduce_min(example) # find the minimum\n",
    "tf.reduce_max(example) # find the maximum\n",
    "tf.reduce_mean(example) # find the mean\n",
    "tf.reduce_sum(example) # find the sum\n",
    "\n",
    "\n",
    "\n",
    "# Other potentially useful tf functions\n",
    "ex_var = tf.math.reduce_variance(tf.cast(example, dtype=tf.float32))\n",
    "ex_std = tf.math.sqrt(ex_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fa44b",
   "metadata": {},
   "source": [
    "#### *Create a model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ebdbcc",
   "metadata": {},
   "source": [
    "In this step, you will define the architecture of your neural network. This involves specifying the number of layers, the number of neurons in each layer, the **activation functions**, and the type of **optimizer** and **loss function** to be used. You should also define the input and output layers, which will match the shape of your input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cb87ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ins_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler, OneHotEncoder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# One-hot encoding with pandas get_dummies()\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m ins_onehot \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(\u001b[43mins_copy\u001b[49m)\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m ins_onehot\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharges\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# split new one-hot data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m ins_onehot[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharges\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ins_copy' is not defined"
     ]
    }
   ],
   "source": [
    "# You may need to take further steps before creating the model\n",
    "\n",
    "# Some imports from sklearn to prepare the data\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# One-hot encoding with pandas get_dummies()\n",
    "ins_onehot = pd.get_dummies(ins_copy)\n",
    "\n",
    "X = ins_onehot.drop([\"charges\"], axis=1) # split new one-hot data\n",
    "y = ins_onehot[\"charges\"]\n",
    "\n",
    "\n",
    "# Create column transfer\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # scale numerical values from 0-1\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n",
    ")\n",
    "\n",
    "# [ RECREATE DATA FEATURES/LABELS IF NECESSARY ]\n",
    "\n",
    "ct.fit(X_train) # fit column transformer to train data\n",
    "\n",
    "\n",
    "# Transform training and test data with normalization (MinMaxScaler & OneHot...)\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test) # must test on same data as trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a76dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "\n",
    "# If you desire reproducible results\n",
    "tf.random.set_seed(42) # any number\n",
    "\n",
    "# Define any callbacks\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# Build suitable model\n",
    "ins_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99af136",
   "metadata": {},
   "source": [
    "#### *Compile the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c53f72",
   "metadata": {},
   "source": [
    "After creating the model, you should compile it by specifying the optimizer, the loss function, and the **metrics** to be used during training. This step prepares the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a746db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "ins_model.compile(loss = tf.keras.optimizers.mae,\n",
    "                 optimizer = tf.keras.optimizers.Adam(lr=0.001),\n",
    "                 metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7cd7b",
   "metadata": {},
   "source": [
    "#### *Fit the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44063061",
   "metadata": {},
   "source": [
    "Once the model is compiled, you can fit it to the training data by specifying the number of epochs, the **batch size**, and the validation split. This step involves training the model on the training data, evaluating its performance on the validation data, and adjusting the model parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model - capture history object to visualize performance afterward \n",
    "history = ins_model.fit(X_train_normal,\n",
    "                       y_train, \n",
    "                        epochs=100,\n",
    "                       callbacks=[early_stop_cb])\n",
    "\n",
    "# Plot history (loss/training curve)\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad5b18",
   "metadata": {},
   "source": [
    "#### *Evaluate the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970b627",
   "metadata": {},
   "source": [
    "After training the model, you should evaluate its performance on the test data. This step involves making predictions on the test data, calculating the evaluation metrics, and comparing the results with the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_model.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1061ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_model.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd9313",
   "metadata": {},
   "source": [
    "#### *Visualize and plot models*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b2ba9",
   "metadata": {},
   "source": [
    "After training the model, you should evaluate its performance on the test data. This step involves making predictions on the test data, calculating the evaluation metrics, and comparing the results with the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81212874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find layers of a model\n",
    "ins_model.layers # <-- can be indexed , if necessary\n",
    "\n",
    "\n",
    "# Get patterns of a given layer in a network\n",
    "weights, biases = ins_model.layers[0].get_weights()\n",
    "\n",
    "\n",
    "# Shapes\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize models using keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "ins_model.summary()\n",
    "\n",
    "# OR\n",
    "\n",
    "plot_model(ins_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b07e87",
   "metadata": {},
   "source": [
    "#### *Save the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c3b9a",
   "metadata": {},
   "source": [
    "Once you are satisfied with the performance of the model, you should save it for future use. You can save the model in the **SavedModel** or **HDF5**/.h5 format, which will allow you to reuse the model for inference or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e051bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in .h5 format\n",
    "ins_model.save('model_name.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b409d38",
   "metadata": {},
   "source": [
    "## *(2) Basic Neural Network Classification*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83305d1",
   "metadata": {},
   "source": [
    "### ***(a) Binary Classification***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4bc28",
   "metadata": {},
   "source": [
    "#### *Gather, format, and split data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084c1bf",
   "metadata": {},
   "source": [
    "This step involves gathering your data and formatting it in a way that the neural network can process it. You should also **split the data** into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03,\n",
    "                    random_state=42)\n",
    "\n",
    "# SEE BELOW FOR DATA SPLIT (POST VISUALIZATION/EXPLORATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdc0d4",
   "metadata": {},
   "source": [
    "#### *Visualize the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcada44",
   "metadata": {},
   "source": [
    "Once you have formatted the data, it's important to visualize it to gain insights and identify patterns. You should use techniques such as scatter plots, histograms, and correlation matrices to understand the relationships between the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ceeefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the features and labels\n",
    "X[:20], y[:50]\n",
    "\n",
    "\n",
    "\n",
    "# Visualize as a table and with matplotlib\n",
    "import pandas as pd\n",
    "circles = pd.DataFrame({\"X0\":X[:, 0],\n",
    "                        \"X1\": X[:, 1],\n",
    "                        \"label\": y})\n",
    "circles\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:,1], c=y, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "\n",
    "\n",
    "# Check lengths and shapes\n",
    "X.shape, y.shape\n",
    "\n",
    "len(X), len(y)\n",
    "\n",
    "X[0], y[0]\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, y_train = X[:800], y[:800]\n",
    "X_test, y_test = X[800:], y[800:]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfb10a",
   "metadata": {},
   "source": [
    "#### *Specify callbacks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8bed0",
   "metadata": {},
   "source": [
    "**Callbacks** are functions that are called during training at specific points to perform actions such as saving the model, adjusting the **learning rate**, or stopping training if the model is not improving. You should specify the appropriate callbacks for your binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_path = \"model_checkpoint.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                   save_weights_only=True,\n",
    "                                   save_best_only=True,\n",
    "                                   monitor='val_loss',\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "# EarlyStopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a callback that stops the training when val_loss doesn't improve for 5 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "\n",
    "# LearningRateScheduler\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n",
    "\n",
    "\n",
    "# SEE 'CALLBACKS' IN NON-PROBLEM SPECIFICS TASKS SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7e1ec",
   "metadata": {},
   "source": [
    "#### *Create/import model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8f7bd",
   "metadata": {},
   "source": [
    "In this step, you will create or import the architecture of your neural network. This involves specifying the number of layers, the number of neurons in each layer, the activation functions, and the type of optimizer and loss function to be used. You should also define the input and output layers, which will match the shape of your input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create the model\n",
    "bin_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"), # <-- non-linearity\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6279546",
   "metadata": {},
   "source": [
    "#### *Compile and fit*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f493a",
   "metadata": {},
   "source": [
    "After creating the model, you should compile it by specifying the optimizer, the loss function, and the metrics to be used during training. This step prepares the model for training. You can then fit the model to the training data by specifying the number of epochs, the batch size, and the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06cd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "bin_model.compile(loss = tf.keras.losses.BinaryCrossentropy(), # <-- binary\n",
    "                 optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "bin_model.fit(X_train, y_train, epochs-100,\n",
    "              callbacks=[lr_scheduler], verbose=True)\n",
    "\n",
    "# Plot to visualize loss curves\n",
    "pd.DataFrame(history.history).plot() # <-- plot 'loss' and 'accuracy'\n",
    "plt.title(\"Model Loss Curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefbd8c",
   "metadata": {},
   "source": [
    "#### *Ensure introduction of non-linearity*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40614aec",
   "metadata": {},
   "source": [
    "For binary classification problems, it's important to introduce non-linearity in the output layer using the **sigmoid activation** function. This ensures that the output of the model is between 0 and 1, which can be interpreted as the probability of the input belonging to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa436b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has been commented on above in the 'Create/import model' section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dd1a3",
   "metadata": {},
   "source": [
    "#### *Learning Rate Scheduler callback*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2754a9",
   "metadata": {},
   "source": [
    "The learning rate is an important hyperparameter that determines the step size of the optimizer during training. You can use the **Learning Rate Scheduler*** callback to adjust the learning rate during training to improve the model's performance. You can also plot the learning rate using the *tf.range* and *plt.semilogx* functions to visualize the effect of the learning rate on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fee3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See 'Specify Callbacks' section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c776049",
   "metadata": {},
   "source": [
    "#### *Evaluate using accuracy, precision/recall, F1 score, confusion matrix, and classification report*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2d528",
   "metadata": {},
   "source": [
    "Once the model is trained, you should evaluate its performance on the test data. Binary classification problems can be evaluated using metrics such as **accuracy**, **precision**, **recall**, **F1 score**, and the **confusion matrix**. You can also generate a **classification report** to get a detailed summary of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE 'NON-PROBLEM SPECIFIC TASKS' SECTION FOR CLF METRICS CODE SNIPPETS #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b52a1",
   "metadata": {},
   "source": [
    "### ***(b) Multi-class Classification***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94d460",
   "metadata": {},
   "source": [
    "#### *Import data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a6425",
   "metadata": {},
   "source": [
    "This step involves importing the data in a format that the neural network can process. You should ensure that the data is in a format that can be used by TensorFlow, such as CSV, JSON, or NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# In this particular example, the dataset comes pre-sorted (split)\n",
    "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d7a04",
   "metadata": {},
   "source": [
    "#### *Split into train/test/val*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d34f8d",
   "metadata": {},
   "source": [
    "Once you have imported the data, you should split it into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune the hyperparameters, and the test set is used to evaluate the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cd733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See above cell, where this was done along with the data import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af48f6",
   "metadata": {},
   "source": [
    "#### *Check shapes + some examples*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571bb25",
   "metadata": {},
   "source": [
    "After splitting the data, you should **check the shapes** of the input and output data. You should also print out some examples of the input and output data to ensure that the data is being correctly processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of a single example\n",
    "train_data[0].shape, train_labels[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7fea7",
   "metadata": {},
   "source": [
    "#### *Visualize*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5177b0",
   "metadata": {},
   "source": [
    "Visualizing the data can help to gain insights and identify patterns in the data. You should use techniques such as scatter plots, histograms, and correlation matrices to understand the relationships between the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single sample\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_data[0]);\n",
    "\n",
    "\n",
    "# PLot an example image and its label\n",
    "index_of_choice = 200\n",
    "plt.imshow(train_data[17], cmap=plt.cm.binary)\n",
    "plt.title(class_names[train_labels[index_of_choice]])\n",
    "\n",
    "\n",
    "# Plot multiple random images of the data\n",
    "import random\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "for i in range(4):\n",
    "  ax = plt.subplot(2,2,i+1)\n",
    "  rand_index = random.choice(range(len(train_data)))\n",
    "  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n",
    "  plt.title(class_names[train_labels[rand_index]])\n",
    "  plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556239a7",
   "metadata": {},
   "source": [
    "#### *Create list of class_names*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ed22c",
   "metadata": {},
   "source": [
    "For multi-class classification problems, you should create a **list of class_names** to map the output classes to their corresponding names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd21ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of class_names (human-readable)\n",
    "class_names=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085eff34",
   "metadata": {},
   "source": [
    "#### *Build NN architecture*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945d5f4",
   "metadata": {},
   "source": [
    "The architecture of the neural network for multi-class classification problems will be similar to binary classification problems, but the output layer will use the **softmax activation** function. You should also use the **CategoricalCrossentropy loss** function to compute the loss during training. It's important to scale (normalize) the data to ensure that the inputs are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c279143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure that your data is scaled (normalized) before running through model\n",
    "train_data_norm = train_data / 255.0\n",
    "test_data_norm = test_data / 255.0\n",
    "\n",
    "# Check the min and max values of the scaled training data\n",
    "train_data_norm.min(), train_data_norm.max()\n",
    "\n",
    "# Should be (0.0, 1.0)\n",
    "\n",
    "\n",
    "# Now that your data is normalized, build a suitable model for your problem:\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "cat_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535d7a3",
   "metadata": {},
   "source": [
    "#### *Train and export model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09df8e",
   "metadata": {},
   "source": [
    "After creating the model, you should train it using the training and validation sets. You can then export the model to a format that can be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "cat_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Fit & train model\n",
    "cat_model_history = cat_model.fit(train_data_norm, train_labels, epochs=10,\n",
    "                          validation_data=(test_data_norm, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea208f4b",
   "metadata": {},
   "source": [
    "#### *Plot LR decay for ideal rate*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1848700",
   "metadata": {},
   "source": [
    "The learning rate is an important hyperparameter that determines the step size of the optimizer during training. You can use a **learning rate decay function** to adjust the learning rate during training to improve the model's performance. You can also **plot the learning rate decay** to visualize the effect of the learning rate on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rate decay curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lrs = 1e-3 * 10**(tf.range(40)/20)\n",
    "plt.semilogx(lrs, fit_lr_history.history[\"loss\"])\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Find the Ideal LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed7905",
   "metadata": {},
   "source": [
    "#### *Evaluate using classification metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620c1f4",
   "metadata": {},
   "source": [
    "Once the model is trained, you should evaluate its performance on the test data. Multi-class classification problems can be evaluated using metrics such as accuracy, precision, recall, F1 score, and the confusion matrix. You can also generate a classification report to get a detailed summary of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE 'NON-PROBLEM SPECIFIC TASKS' SECTION FOR CLF METRICS CODE SNIPPETS #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2fe8f",
   "metadata": {},
   "source": [
    "#### *Train for longer OR change NN architecture*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20348c43",
   "metadata": {},
   "source": [
    "If the model's performance is not satisfactory, you can train the model for longer or change the architecture of the neural network. You can experiment with different hyperparameters and architectures to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE 'PREVENT OVERFITTING' IN NON-PROBLEM SPECIFIC TASKS # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd72cab",
   "metadata": {},
   "source": [
    "## *(3) Convolutional Neural Networks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3890464e",
   "metadata": {},
   "source": [
    "### ***(a) Binary Image Classification***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9294637",
   "metadata": {},
   "source": [
    "#### *Import dataset or set variables for file/directory paths (train/test/validation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4f50c",
   "metadata": {},
   "source": [
    "In addition to importing the dataset or setting variables for the file/directory paths, you should also check if there are any missing or corrupted files. You may also want to consider splitting your data into training, validation, and test sets to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n",
    "\n",
    "# Unzip the downloaded file\n",
    "zip_ref = zipfile.ZipFile(\"pizza_steak.zip\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "\n",
    "\n",
    "# Navigate through dirs/files\n",
    "!ls pizza_steak\n",
    "!ls pizza_steak/train\n",
    "!ls pizza_steak/train/steak\n",
    "\n",
    "\n",
    "\n",
    "# Walk through dirs and list number of files\n",
    "import os\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(\"pizza_steak\"):\n",
    "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
    "\n",
    "\n",
    "\n",
    "# Another way to find out how many imgs are in a file\n",
    "num_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))\n",
    "num_pizza_images_train = len(os.listdir(\"pizza_steak/train/pizza\"))\n",
    "\n",
    "num_steak_images_train, num_pizza_images_train\n",
    "\n",
    "\n",
    "\n",
    "# Get the classnames programatically\n",
    "import pathlib\n",
    "import numpy as np\n",
    "data_dir = pathlib.Path(\"pizza_steak/train\")\n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
    "# create list of class names from the subdirectories\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e345edc",
   "metadata": {},
   "source": [
    "#### *Data preprocessing*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db65507",
   "metadata": {},
   "source": [
    " In addition to normalizing pixel values between 0 and 1, you should also consider **resizing your images** to a fixed size to ensure consistency across all images. You can use tf.image.resize() for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our images before carrying out any permanent actions\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "def view_random_image(target_dir, target_class):\n",
    "  # Setup the target directory (view images from here)\n",
    "  target_folder = target_dir+target_class\n",
    "\n",
    "  # Get a random image path\n",
    "  random_image = random.sample(os.listdir(target_folder), 1)\n",
    "  print(random_image)\n",
    "\n",
    "  # Read in the image and plot it with matplotlib\n",
    "  img = mpimg.imread(target_folder+\"/\"+random_image[0])\n",
    "  plt.imshow(img)\n",
    "  plt.title(target_class)\n",
    "  plt.axis(\"off\");\n",
    "\n",
    "  print(f\"Image shape: {img.shape}\") # show shape of the image\n",
    "\n",
    "  return img\n",
    "\n",
    "\n",
    "\n",
    "# View a random image from the training dataset\n",
    "img = view_random_image(target_dir=\"pizza_steak/train/\",\n",
    "                        target_class=\"pizza\")\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c5af9",
   "metadata": {},
   "source": [
    "#### *Import from directories and turn into batches*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbd138",
   "metadata": {},
   "source": [
    "You can use the **tf.keras.preprocessing.image.ImageDataGenerator** class to import images from directories and turn them into batches. This class also allows you to perform **data augmentation** techniques such as random rotations, flips, and shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# < WITHOUT DATA AUGMENTATION > #\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# set seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Preprocess data (get all the pixel values between 0-1; also called scaling/normalization)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    " # Setup paths to our data directories\n",
    "train_dir = \"/content/pizza_steak/train\"\n",
    "test_dir = \"/content/pizza_steak/test\"\n",
    "\n",
    "\n",
    "# Import data from directories and turn it into batches\n",
    "train_data = train_datagen.flow_from_directory(directory=train_dir,\n",
    "                                             batch_size=32,\n",
    "                                            target_size=(224, 224),\n",
    "                                            class_mode=\"binary\", # <---\n",
    "                                            seed=42)\n",
    "\n",
    "test_data = train_datagen.flow_from_directory(directory=test_dir,\n",
    "                                               batch_size=32,\n",
    "                                               target_size=(224,224),\n",
    "                                               class_mode=\"binary\", # <---\n",
    "                                               seed=42)\n",
    "\n",
    "\n",
    "\n",
    "# Get a sample of a training data batch\n",
    "images, labels = train_data.next()\n",
    "len(images), len(labels)\n",
    "\n",
    "# How many batches are there?\n",
    "len(train_data)\n",
    "\n",
    "1500/32 # batches in the dataset (rounded up)\n",
    "\n",
    "\n",
    "\n",
    "# < ADDING DATA AUGMENTATION (SEE NON-SPECIFIC TASKS SECTION, TOO) > #\n",
    "\n",
    "# Create ImageDataGenerator training instance with data augmentation\n",
    "train_datagen_aug = ImageDataGenerator(rescale=1/255.,\n",
    "                                       rotation_range=0.2,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       horizontal_flip=True)\n",
    "\n",
    "train_datagen_aug_shuff = train_datagen_aug.flow_from_directory(train_dir,\n",
    "                                                          target_size=(224,224),\n",
    "                                                          class_mode=\"binary\",\n",
    "                                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b61b05",
   "metadata": {},
   "source": [
    "#### *Build CNN model - ensure shapes are correct*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435f897",
   "metadata": {},
   "source": [
    " When building a CNN model, you need to make sure that the shapes of the input and output layers are correct. You should also consider using **padding** to preserve the spatial dimensions of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce159159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN model (same as the tiny VGG on the CNN explainer website)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=10,\n",
    "                           kernel_size=3,\n",
    "                           activation=\"relu\",\n",
    "                           input_shape=(224,224,3)),\n",
    "    tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2,\n",
    "                              padding=\"valid\"),\n",
    "    tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Alternatively...\n",
    "\n",
    "# Create the model (this will be the baseline - 3 layer CNN)\n",
    "cnn = Sequential([\n",
    "    Conv2D(filters=10,\n",
    "           kernel_size=3,\n",
    "           strides=1,\n",
    "           padding=\"valid\",\n",
    "           activation=\"relu\",\n",
    "           input_shape=(224,224,3)),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    Flatten(),\n",
    "    Dense(1, activation=\"sigmoid\") # binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbe3b2",
   "metadata": {},
   "source": [
    "#### *Compile and fit model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a6898",
   "metadata": {},
   "source": [
    "When compiling the model, you should specify the loss function, optimizer, and metrics to be used during training. For binary image classification, you should use **binary cross-entropy** as the loss function and **Adam optimizer**. You should also consider using **early stopping** to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile our CNN\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(train_data, # train_data Object creates X/y for us\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=len(train_data),\n",
    "                    validation_data=valid_data,\n",
    "                    validation_steps=len(valid_data))\n",
    "\n",
    "\n",
    "# Plot loss curves (SEE NON-SPECIFIC TASKS CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620ac35",
   "metadata": {},
   "source": [
    "#### *Evaluate the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becddf20",
   "metadata": {},
   "source": [
    "To evaluate the performance of your model, you can use metrics such as accuracy, precision, recall, and F1-score. You should also consider using a confusion matrix and classification report to get a more detailed analysis of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in an example image\n",
    "!wget [URL]\n",
    "\n",
    "# Visualize the image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "steak = mpimg.imread(\"03-steak.jpeg\")\n",
    "plt.imshow(steak)\n",
    "plt.axis(False)\n",
    "\n",
    "steak.shape\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the custom image\n",
    "model.predict(tf.expand_dims(steak, axis=0))\n",
    "\n",
    "\n",
    "\n",
    "# OPTIONAL: Functionize the loading and prepping of custom images\n",
    "def load_and_prep_image(filename, img_shape=224):\n",
    "  \"\"\"\n",
    "  Reads an image from filename, turns it into a tensor and reshapes it \n",
    "  to (img_shape, img_shape, color_channels)\n",
    "  \"\"\"\n",
    "  # Read in the image\n",
    "  img = tf.io.read_file(filename)\n",
    "\n",
    "  # Decode the read file into a tensor\n",
    "  img = tf.image.decode_image(img)\n",
    "\n",
    "  # Resize the image\n",
    "  img = tf.image.resize(img, size=[img_shape, img_shape])\n",
    "\n",
    "  # Rescale to get all of the image values between 0 & 1\n",
    "  img = img/255.\n",
    "\n",
    "  return img\n",
    "\n",
    "\n",
    "# Load and preprocess custom image\n",
    "steak = load_and_prep_image(\"03-steak.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6beeeb7",
   "metadata": {},
   "source": [
    "#### *Consider improvements*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72516f62",
   "metadata": {},
   "source": [
    "You can consider adding more convolutional layers and increasing the number of filters in each layer to capture more complex features in the images. You can also add a fully connected layer after flattening to further refine the features. Additionally, you can use data augmentation techniques to generate more training data and reduce overfitting. Finally, you can add **regularization** layers such as **MaxPool2D** to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e0aee",
   "metadata": {},
   "source": [
    "### ***(b) Multi-class Image Classification***\n",
    "\n",
    "*(Quite similar to the above, with a few important distinctions. The steps are explored below in slightly less depth than the other Processes.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffbb0a9",
   "metadata": {},
   "source": [
    "(i) Import the dataset or set variables for file/directory paths for the training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c80b6d",
   "metadata": {},
   "source": [
    "(ii) Assign the training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc10d6",
   "metadata": {},
   "source": [
    "(iii) Check the file sizes and formats to ensure that they are compatible with the neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049de3c4",
   "metadata": {},
   "source": [
    "(iv) Get examples and inspect them programmatically to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1d72b",
   "metadata": {},
   "source": [
    "(v) Preprocess the images by **normalizing** the pixel values to be between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413de41",
   "metadata": {},
   "source": [
    "(vi) Import the images from the directories and turn them into batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f225130",
   "metadata": {},
   "source": [
    "(vii) Build the CNN model, ensuring that the input shape and output shape are correctly specified for the number of classes in the dataset, and the activation function for the output layer is *softmax*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dda6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build out an appropriate CNN architecture\n",
    "cnn = Sequential([\n",
    "    Conv2D(10, 3, activation=\"relu\", input_shape=(224, 224,3)),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=2),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"softmax\") # <-- 'softmax' instead of 'sigmoid'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd50ae6",
   "metadata": {},
   "source": [
    "(viii) Compile the model, specifying the loss function as *CategoricalCrossentropy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss=CategoricalCrossentropy(),\n",
    "                     optimizer=Adam(),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d6d9a",
   "metadata": {},
   "source": [
    "(ix) Fit the model and evaluate it using metrics such as accuracy, precision/recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96afdb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_hist = cnn.fit(train_data, \n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=len(train_data),\n",
    "                    validation_data=test_data,\n",
    "                    validation_steps=len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fac578",
   "metadata": {},
   "source": [
    "(x) Visualize the accuracy and loss curves over the epochs to ensure that the model is not overfitting or underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f773a",
   "metadata": {},
   "source": [
    "(xi) Make and visualize predictions on the test set to assess the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14444987",
   "metadata": {},
   "source": [
    "(xii) Consider using transfer learning by importing a pre-trained model, such as **VGG16** or **ResNet**, and adding additional layers to fine-tune the model for the specific classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23484044",
   "metadata": {},
   "source": [
    "## *(4) Transfer Learning - Feature Extraction*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f67e0",
   "metadata": {},
   "source": [
    "#### *Import the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e369b8",
   "metadata": {},
   "source": [
    "Load the data using standard file I/O libraries or functions. The data should be formatted in a way that is compatible with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f501ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Download the data (10% of 10 Food Classes repo from Food101 paper)\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "\n",
    "# Unzip the downloaded file\n",
    "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Walk through directory and list number of files\n",
    "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
    "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0381c99d",
   "metadata": {},
   "source": [
    "#### *Prepare and split the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377074d",
   "metadata": {},
   "source": [
    "Prepare the data by converting it to a format that is compatible with TensorFlow. Then, split the data into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddec2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data inputs\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMG_SHAPE = (224,224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "train_dir = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = \"10_food_classes_10_percent/test/\"\n",
    "\n",
    "train_data_gen = ImageDataGenerator(rescale=1/255.)\n",
    "test_data_gen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "print(\"Training images:\")\n",
    "train_data_10_percent = train_data_gen.flow_from_directory(train_dir,\n",
    "                                                           target_size=IMG_SHAPE,\n",
    "                                                           batch_size=BATCH_SIZE,\n",
    "                                                           class_mode=\"categorical\")\n",
    "\n",
    "print(\"Testing images:\")\n",
    "test_data = test_data_gen.flow_from_directory(test_dir,\n",
    "                                              target_size=IMG_SHAPE,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eccecc",
   "metadata": {},
   "source": [
    "#### *Preprocess the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7b63e",
   "metadata": {},
   "source": [
    "Preprocess the data by normalizing the pixel values and resizing the images, if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08807d2",
   "metadata": {},
   "source": [
    "#### *Setup some callbacks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409b42e",
   "metadata": {},
   "source": [
    "Use TensorBoard callback to monitor and visualize model training progress, **Model checkpoint callback** to save the best model during training, and Early stopping callback to stop training the model when it's no longer improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example TensorBoard callback (functionized for each new model)\n",
    "import datetime\n",
    "\n",
    "def create_tb_callback(dir_name, experiment_name):\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d, %H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e52418",
   "metadata": {},
   "source": [
    "#### *Select a model architecture from TF Hub*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1a067",
   "metadata": {},
   "source": [
    "Choose a pre-trained model from **TensorFlow Hub** that is compatible with the task:\n",
    "\n",
    "*https://thfub.dev/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94ec63",
   "metadata": {},
   "source": [
    "#### *Save model url(s) to a variable*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503c6f2",
   "metadata": {},
   "source": [
    "Save the URL of the pre-trained model to a variable that can be used to instantiate the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcfe022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models selected for example comparison\n",
    "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "\n",
    "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
    "\n",
    "mobilenet_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3206a",
   "metadata": {},
   "source": [
    "#### *Create a pretrained model instance with hub.KerasLayer(...)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7116c",
   "metadata": {},
   "source": [
    "Instantiate the pre-trained model using the **hub.KerasLayer** function. This function loads the model and freezes its weights so that they are not updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary features imported\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Example create_model function to create models from TF Hub URLs\n",
    "def create_model(model_url, num_classes=10):\n",
    "  \"\"\"\n",
    "  Takes a TF Hub URL and creates a keras.Sequential model with it.\n",
    "\n",
    "  Args:\n",
    "    model_url (str): A TensorFlow Hub feature extraction URL.\n",
    "    num_classes (int): Number of output neurons in the output layer,\n",
    "      should be equal to number of target classes; default 10.\n",
    "\n",
    "  Returns:\n",
    "    An uncompiled keras.Sequential model with _model_url as feature \n",
    "    extractor layer and Dense output layer with num_classes output neurons.\n",
    "  \"\"\"\n",
    "\n",
    "  # Download pretrained model and save it as a Keras layer\n",
    "  feature_extractor_layer = hub.KerasLayer(model_url,\n",
    "                                           trainable=False, # freeze the already learned patterns\n",
    "                                           name=\"feature_extraction_layer\",\n",
    "                                           input_shape=IMG_SHAPE+(3,))\n",
    "  # Create our own model\n",
    "  model = tf.keras.Sequential([\n",
    "      feature_extractor_layer,\n",
    "      layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "# Create Resnet model\n",
    "resnet_model = create_model(resnet_url,\n",
    "                            num_classes=train_data_10_percent.num_classes)\n",
    "\n",
    "# EfficientNet\n",
    "eff_net_model = create_model(efficientnet_url,\n",
    "                            num_classes=train_data_10_percent.num_classes)\n",
    "\n",
    "\n",
    "# MobileNet\n",
    "mobilenet_model=create_model(model_url=mobilenet_url,\n",
    "                             num_classes=train_data_10_percent.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e567573",
   "metadata": {},
   "source": [
    "#### *Compile and fit model with any callbacks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c6801",
   "metadata": {},
   "source": [
    "Compile the model by specifying the loss function, optimizer, and metrics. Then, fit the model to the training data using the fit method, and specify any relevant callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45601eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit ResNet\n",
    "resnet_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                     optimizer=tf.keras.optimizers.Adam(),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "resnet_hist = resnet_model.fit(train_data_10_percent, epochs=5,\n",
    "                               steps_per_epoch=len(train_data_10_percent),\n",
    "                               validation_data=test_data,\n",
    "                               validation_steps=len(test_data),\n",
    "                               callbacks=[create_tb_callback(dir_name=\"tensorflow_hub\",\n",
    "                                                             experiment_name=\"resnet50v2\"\n",
    "                                                             )])\n",
    "\n",
    "\n",
    "# Compile and fit EfficientNet\n",
    "eff_net_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                     optimizer=tf.keras.optimizers.Adam(),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "eff_net_hist = eff_net_model.fit(train_data_10_percent, epochs=10,\n",
    "                               steps_per_epoch=len(train_data_10_percent),\n",
    "                               validation_data=test_data,\n",
    "                               validation_steps=len(test_data),\n",
    "                               callbacks=[create_tb_callback(dir_name=\"tensorflow_hub\",\n",
    "                                                             experiment_name=\"efficientnetb0\"\n",
    "                                                             )])\n",
    "\n",
    "\n",
    "\n",
    "# Compile and fit MobileNet\n",
    "mobilenet_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                        optimizer=tf.keras.optimizers.Adam(),\n",
    "                        metrics=[\"accuracy\"])\n",
    "\n",
    "mobilenet_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                        optimizer=tf.keras.optimizers.Adam(),\n",
    "                        metrics=[\"accuracy\"])\n",
    "\n",
    "from tensorflow.python import train\n",
    "mobilenet_hist = mobilenet_model.fit(train_data_10_percent, epochs=10,\n",
    "                                     steps_per_epoch=len(train_data_10_percent),\n",
    "                                     validation_data=test_data,\n",
    "                                     validation_steps=len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f218e43",
   "metadata": {},
   "source": [
    "#### *Visualize results*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca073c6b",
   "metadata": {},
   "source": [
    " Visualize the results of the trained model, such as accuracy and loss, using appropriate plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See model summary\n",
    "[model_name].summary()\n",
    "\n",
    "\n",
    "\n",
    "# Plot the loss curves\n",
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Plots the loss curves from the training and validation models from the history\n",
    "  attribute of a trained model.\n",
    "  \"\"\"\n",
    "\n",
    "  accuracy = history.history[\"accuracy\"]\n",
    "  val_accuracy = history.history[\"val_accuracy\"]\n",
    "\n",
    "  loss = history.history[\"loss\"]\n",
    "  val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "  epochs = range(len(history.history[\"loss\"]))\n",
    "\n",
    "  # Plot loss\n",
    "  plt.plot(epochs, loss, label=\"training_loss\")\n",
    "  plt.plot(epochs, val_loss, label=\"val_loss\")\n",
    "  plt.title(\"Loss\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.grid(False)\n",
    "  plt.legend()\n",
    "\n",
    "  # Plot the accuracy\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label=\"training_accuracy\")\n",
    "  plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n",
    "  plt.title(\"Accuracy\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.grid(False)\n",
    "  plt.legend();\n",
    "\n",
    "\n",
    "\n",
    "# View specific model layers and weights\n",
    "[model_name].layers[0].weights\n",
    "\n",
    "\n",
    "\n",
    "# View and compare model histories with TensorBaord\n",
    "!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n",
    "  --name \"EfficientNetB0 vs. ResNet50v2\" \\\n",
    "  --description \"Comparing two different TF Hub feature extraction architectures using 10% of the training data.\" \\\n",
    "  --one_shot\n",
    "\n",
    "# < ADDITIONAL TENSORBOARD INFO > #\n",
    "\n",
    "# See what TensorBoard experiments you have\n",
    "!tensorboard dev list\n",
    "\n",
    "# Deleting an experiment\n",
    "!tensorboard dev delete --experiment_id [paste exp id]\n",
    "\n",
    "# Confirm deletion by rechecking the experiments list\n",
    "!tensorboard dev list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f82ff4",
   "metadata": {},
   "source": [
    "#### *Review callback data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94deb076",
   "metadata": {},
   "source": [
    "Review the data generated by the callbacks, such as loss and accuracy over epochs, to gain insights into the training process and identify potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5b7eb",
   "metadata": {},
   "source": [
    "## *(5) Transfer Learning - Fine-Tuning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482456c0",
   "metadata": {},
   "source": [
    "#### *Import and preprocess the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 10% of the training data\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "\n",
    "unzip_data(\"10_food_classes_10_percent.zip\") # from helper_functions\n",
    "\n",
    "\n",
    "\n",
    "# Check out how many images and subdirectories are in the dataset\n",
    "walk_through_dir(\"10_food_classes_10_percent\")\n",
    "\n",
    "# Create training and test directory paths\n",
    "train_dir = \"10_food_classes_10_percent/train\"\n",
    "test_dir = \"10_food_classes_10_percent/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a73b0",
   "metadata": {},
   "source": [
    "*(a) **tf.keras.preprocessing.image_dataset_from_directory** or a similar method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf343b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode=\"categorical\",\n",
    "                                                                            batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode=\"categorical\",\n",
    "                                                                            batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea66d2",
   "metadata": {},
   "source": [
    "*(b) Preprocess the data by rescaling, resizing, and possibly applying data augmentation using **tf.keras.preprocessing.image.ImageDataGenerator***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# Create data augmentation stage with horizontal flipping, rotations, zoomz, etc.\n",
    "data_augmentation = keras.Sequential([\n",
    "    preprocessing.RandomFlip(\"horizontal\"),\n",
    "    preprocessing.RandomRotation(0.2),\n",
    "    preprocessing.RandomZoom(0.2),\n",
    "    preprocessing.RandomHeight(0.2),\n",
    "    preprocessing.RandomWidth(0.2)\n",
    "    # preprocessing.Rescale(1./255) # keep for models like ResNet50v2 (Effnet has this built in)\n",
    "], name = \"data_augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdc6ae",
   "metadata": {},
   "source": [
    "*(c) Split the data into training and validation sets. (See Step (a) if dealing with images)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadd7fa",
   "metadata": {},
   "source": [
    "#### *Choose a pre-trained model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab5b23",
   "metadata": {},
   "source": [
    "*(a) Choose a pre-trained model from tf.keras.applications or TensorFlow Hub that is suitable for the problem at hand.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b0f24",
   "metadata": {},
   "source": [
    "*(b) Import the pre-trained model and set its weights to be non-trainable.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a87aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the base model with tf.keras.applications\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "# 2. Freeze the base model (so the underlying pretrained patterns aren't updated)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cd51e",
   "metadata": {},
   "source": [
    "#### *Build the fine-tuning model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a292d75",
   "metadata": {},
   "source": [
    "*(a) Create a new **tf.keras.Sequential** or **tf.keras.Model** that includes the pre-trained model as a layer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a5da4",
   "metadata": {},
   "source": [
    "*(b) Add new trainable layers on top of the pre-trained model to adapt it to the new problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924dd0e",
   "metadata": {},
   "source": [
    "*(c) Compile the model with an appropriate loss function, optimizer, and metrics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2139c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create inputs into our model\n",
    "inputs = tf.keras.layers.Input(shape=(224,224,3), name=\"input_layer\")\n",
    "\n",
    "# 4. If using a model like ResNet50v2, you will need to normalize inputs\n",
    "x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "\n",
    "# 5. Pass the inputs to the base_model\n",
    "x = base_model(inputs)\n",
    "print(f\"Shape after passing inputs through base model: {x.shape}\")\n",
    "\n",
    "# 6. Average pool the outputs of the base_model (aggregate all the most important information, reduce computations)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "print(f\"Shape after GlobalAveragePooling2D: {x.shape}\")\n",
    "\n",
    "# 7. Create the output activation layer\n",
    "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "# 8. Combine the inputs with the outputs into a model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# 9. Compile the model\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 10. Fit the model and save its history\n",
    "history = model.fit(train_data_10_percent, epochs=5,\n",
    "                        steps_per_epoch=len(train_data_10_percent),\n",
    "                        validation_data=test_data,\n",
    "                        validation_steps=int(0.25 * len(test_data)),\n",
    "                        callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
    "                                                               experiment_name=\"10_percent_feature_extraction\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc39d3",
   "metadata": {},
   "source": [
    "#### *Fine-tune the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c350cd5",
   "metadata": {},
   "source": [
    "*(a) Train the model on the new data, possibly using a learning rate scheduler, early stopping, or other callbacks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853cfef0",
   "metadata": {},
   "source": [
    "*(b) Gradually unfreeze some of the layers in the pre-trained model and continue training to improve performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef671e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View layers\n",
    "model.layers\n",
    "\n",
    "\n",
    "\n",
    "# Check if trainable\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)\n",
    "    \n",
    "    \n",
    "    \n",
    "# To begin fine-tuning, set the desired layers of base_model to be trainable\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the other layers\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Recompile after adjusting layers\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # typically, you should lower learning rate by 10x when fine-tuning\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Confirm changes were successful\n",
    "for layer_number, layer in enumerate(model_2.layers[2].layers):\n",
    "  print(layer_number, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3939d",
   "metadata": {},
   "source": [
    "#### *Evaluate and visualize the results*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa57cd6",
   "metadata": {},
   "source": [
    "*(a) Evaluate the performance of the fine-tuned model on the validation set using accuracy, precision, recall, F1-score, or other appropriate metrics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447af24",
   "metadata": {},
   "source": [
    "*(b) Visualize the training and validation accuracy and loss over time to diagnose overfitting or underfitting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f815493",
   "metadata": {},
   "source": [
    "*(c) Visualize some of the model's predictions to gain insights into its behavior.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed0c8a",
   "metadata": {},
   "source": [
    "#### *Consider further improvements*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cfdc95",
   "metadata": {},
   "source": [
    "*(a) Experiment with different pre-trained models or different architectures for the fine-tuning layers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dada82",
   "metadata": {},
   "source": [
    "*(b) Increase the amount of data, apply more aggressive data augmentation, or use techniques such as transfer learning with **data synthesis**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a97c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to incorporate Data Synthesis into a model...\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "\n",
    "# Define the paths to the input and output directories\n",
    "input_dir = \"/path/to/input/images\"\n",
    "output_dir = \"/path/to/output/images\"\n",
    "\n",
    "# Load the pre-trained model from TensorFlow Hub\n",
    "model_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5\"\n",
    "module = hub.KerasLayer(model_url, input_shape=(224, 224, 3))\n",
    "\n",
    "# Define the data generator to read images from the input directory\n",
    "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate the feature vectors for the input images\n",
    "input_data = data_generator.flow_from_directory(input_dir,\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 batch_size=_,\n",
    "                                                 shuffle=[False, True],\n",
    "                                                 class_mode=['binary', 'categorical'])\n",
    "features = module.predict(input_data)\n",
    "\n",
    "# Synthesize new images using the feature vectors\n",
    "for i, feature in enumerate(features):\n",
    "    # Generate a random vector to add noise to the feature vector\n",
    "    noise = np.random.normal(loc=0, scale=0.1, size=feature.shape)\n",
    "    # Add noise to the feature vector\n",
    "    feature += noise\n",
    "    # Decode the feature vector to an image\n",
    "    image = module(np.array([feature]))[0]\n",
    "    image = Image.fromarray(np.uint8(image * 255))\n",
    "    # Save the synthesized image to the output directory\n",
    "    image.save(os.path.join(output_dir, f\"synthesized_{i}.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b028e",
   "metadata": {},
   "source": [
    "*(c) Fine-tune the model for longer or with a different learning rate schedule to achieve better performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385f28e",
   "metadata": {},
   "source": [
    "*(d) Use regularization techniques such as dropout or **batch normalization** to improve the model's generalization ability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fee741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of regularization using Dropout or Batch Normalization...\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the model architecture with regularization, dropout, and batch normalization\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10,\n",
    "                    validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b3e0e",
   "metadata": {},
   "source": [
    "## *(6) Natural Language Processing*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeea082",
   "metadata": {},
   "source": [
    "#### *Import data and any helper functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
    "    \n",
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys, unzip_data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head(5)\n",
    "\n",
    "# To shuffle df...\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56840d0",
   "metadata": {},
   "source": [
    "#### *Visualize and preprocess the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many examples of each class are there?\n",
    "train_df.target.value_counts()\n",
    "len(train_df), len(test_df)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize some random text training samples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5)\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "  _, text, target = row\n",
    "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"---\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# Split into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                              train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                              test_size=0.1, #10% for val split\n",
    "                                                                              random_state=42)\n",
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077dd2a",
   "metadata": {},
   "source": [
    "(a) Clean and preprocess the text data (e.g., remove **stop words**, punctuation, lowercasing, **stemming**, **lemmatization**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. REMOVING STOP WORDS WITH JUST PYTHON AND TENSORFLOW\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "stop_words = set(tf.keras.datasets.imdb.get_word_index().keys()) # retrieve stop words from IMDb dataset\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text_to_word_sequence(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. REMOVING PUNCTUATION AND LOWERCASING\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "\n",
    "# 3. STEMMING WITH NLTK LIBRARY & VANILLA TENSORFLOW\n",
    "\n",
    "from nltk.stem import PorterStemmer # using nltk library (1)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "import tensorflow as tf # using just tensorflow and python (2)\n",
    "from tensorflow.strings import split \n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = tf.keras.preprocessing.text.stem.PorterStemmer()\n",
    "    words = split(text, ' ')\n",
    "    stemmed_words = [stemmer.stem(word.numpy().decode('utf-8')) for word in words]\n",
    "    return tf.constant(' '.join(stemmed_words))\n",
    "\n",
    "\n",
    "\n",
    "# 4. LEMMATIZATION WITH NLTK LIBRARY & TENSORFLOW\n",
    "\n",
    "import nltk # using nltk library (1)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "import tensorflow as tf # alternative using TensorFlow (2)\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords') # Download stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = PorterStemmer() # Initialize stemmer and lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Lowercase the text\n",
    "    \n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text) # Tokenize the text\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word.casefold() not in stop_words]\n",
    "    \n",
    "    # Stem the words\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the words to form a preprocessed text\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870da22a",
   "metadata": {},
   "source": [
    "(b) Split the text into **tokens** using **tokenization** (e.g., using nltk.tokenize, spaCy, or TensorFlow Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization with default parameters (TENSORFLOW)\n",
    "text_vectorizer = TextVectorization(max_tokens=2, # how many words in our vocab (automatically add OOV)\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    split=\"whitespace\",\n",
    "                                    ngrams=None, # create groups of n words\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=None, # how long of sequences\n",
    "                                    pad_to_max_tokens=True)\n",
    "\n",
    "# Setup text vectorization variables\n",
    "max_vocab_length = 10000 # max words in vocabulary\n",
    "max_length = 15 # max length of a sequence (words in a tweet)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)\n",
    "\n",
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "\n",
    "\n",
    "# TextVectorization using NLTK library\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"This is a sample sentence for tokenization. Let's see how it works!\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26ffb8",
   "metadata": {},
   "source": [
    "(c) Convert the data into numerical vectors using:\n",
    "* **Embedding** (e.g., using **tf.keras.layers.Embedding**)\n",
    "* **One-hot encoding** (e.g., using **tf.keras.preprocessing.text.one_hot**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                             output_dim=128,\n",
    "                             embeddings_initializer=\"uniform\",\n",
    "                             input_length=max_length)\n",
    "embedding\n",
    "\n",
    "# TEST: Get a random sentence from the traing set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into a dense vector of fixed size)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed\n",
    "\n",
    "\n",
    "\n",
    "# One-hot encoding labels\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# check what labels look like\n",
    "train_labels_one_hot\n",
    "\n",
    "\n",
    "\n",
    "# Label encode labels\n",
    "# Extract labels (\"target\") and encode them into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.fit_transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.fit_transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "# Check what training labels look like\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6ad68",
   "metadata": {},
   "source": [
    "(d) Pad the sequences to make them of the same length (e.g., using **tf.keras.preprocessing.sequence.pad_sequences**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Assume sequences is a list of integer sequences\n",
    "sequences = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "\n",
    "# Set max_length to the desired maximum length of the sequences\n",
    "max_length = 4\n",
    "\n",
    "# Pad the sequences to make them of the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Embedding\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                             output_dim=128,\n",
    "                             embeddings_initializer=\"uniform\",\n",
    "                             input_length=max_length)\n",
    "\n",
    "# Embed the padded sequences\n",
    "embedded_sequences = embedding(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee85326",
   "metadata": {},
   "source": [
    "(e) Split the data into training, validation, and test sets (if you haven't already)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cfb4e",
   "metadata": {},
   "source": [
    "#### *Build and train the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73973dc8",
   "metadata": {},
   "source": [
    "(a) Specify the model architecture (e.g., using tf.keras.Sequential or tf.keras.Functional API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde00e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a baseline model (NOT RECOMMENDING FOR ACTUAL USE IN EXAM)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save Tensorboard logs\n",
    "SAVE_DIR = \"model_logs\"\n",
    "\n",
    "# Build model with the Functional API\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs) # turn input text into numbers\n",
    "x = embedding(x) # create an embedding of the numerical inputs\n",
    "#x = layers.GlobalAveragePooling1D()(x) # model didn't work w/o this layer\n",
    "x = layers.GlobalMaxPool1D()(x) # performs better than average pooling\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # sigmoid for binary output\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e8ee2",
   "metadata": {},
   "source": [
    "(b) Compile the model (e.g., using tf.keras.Model.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284fb8ad",
   "metadata": {},
   "source": [
    "(c) Fit the model on the training data (e.g., using tf.keras.Model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_history = model.fit(x=train_sentences,\n",
    "                              y=train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                                     experiment_name=\"model_1_dense\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a66714",
   "metadata": {},
   "source": [
    "#### *Evaluate the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e4931",
   "metadata": {},
   "source": [
    "(a) Evaluate the model on the validation and test sets (e.g., using tf.keras.Model.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model.evaluate(val_sentences, val_labels)\n",
    "\n",
    "# Collect set of prediction probabilities\n",
    "model_pred_probs = model.predict(val_sentences)\n",
    "model_pred_probs\n",
    "\n",
    "# Convert model prediction probabilities to same as label format\n",
    "model_preds = tf.squeeze(tf.round(model_pred_probs))\n",
    "model_preds[:20]\n",
    "\n",
    "# Calculate model results\n",
    "model_results = get_scores(val_labels, model_preds) # see Non-Specific Tasks for get_scores code\n",
    "\n",
    "model_results\n",
    "\n",
    "# Compare, if you want\n",
    "np.array(list(model_results.values())) > np.array(list(other_results.values()))\n",
    "\n",
    "print(model_results)\n",
    "print(other_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0683e02",
   "metadata": {},
   "source": [
    "(b) Visualize the model's training history (e.g., using matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5dc45",
   "metadata": {},
   "source": [
    "(c) Visualize learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339af5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary from our text vectorizer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]\n",
    "\n",
    "# Get the weight matrix of embedding layer (numerical representations of each token learned during training)\n",
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()[0]\n",
    "embed_weights\n",
    "\n",
    "\n",
    "\n",
    "# Create embedding files (from TF's word embedding documentation)\n",
    "import io\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "  if index == 0:\n",
    "    continue # skip 0, it's padding\n",
    "  vec = embed_weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "# Download files from colab to open in Projector\n",
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f2c45",
   "metadata": {},
   "source": [
    "#### *Tune the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb6938",
   "metadata": {},
   "source": [
    "(a) Adjust the model hyperparameters (e.g., learning rate, dropout rate, number of layers, number of hidden units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b3318",
   "metadata": {},
   "source": [
    "(b) Try different model architectures (e.g., using convolutional layers or recurrent layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LSTM MODEL\n",
    "\n",
    "# Create an LSTM model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Build model with functional API and print shapes throughout the process\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "\n",
    "x = layers.LSTM(64, return_sequences=True)(x) # when stacking RNN cells, you must set return_sequences = True\n",
    "x = layers.LSTM(64)(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "lstm_model = tf.keras.Model(inputs, outputs, name=\"LSTM_model\")\n",
    "\n",
    "lstm_model.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "lstm_model_history = lstm_model.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences,\n",
    "                                               val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                     \"LSTM_model\")])\n",
    "\n",
    "\n",
    "\n",
    "# 2. GATED RECURRENT UNIT (GRU) MODEL\n",
    "\n",
    "# Build an RNN using the GRU cell\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "\n",
    "x = layers.GRU(64)(x)\n",
    "# x = layers.GRU(64, return_sequences=True) # to stack RNNs on top of one another\n",
    "# x = layers.LSTM(32, return_sequences=True)(x)\n",
    "# x = layers.GRU(64)(x)\n",
    "\n",
    "# layers.Dense(64, activation = \"relu\")(x)\n",
    "# x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "gru_model = tf.keras.Model(inputs, outputs, name=\"model_3\")\n",
    "\n",
    "gru_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "gru_model_history = gru_model.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=10,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                     \"GRU_model\")])\n",
    "\n",
    "\n",
    "\n",
    "# 3. BIDIRECTIONAL LSTM MODEL\n",
    "\n",
    "# Build a bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"input_layer\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # if multiple\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1, activation =\"sigmoid\")(x)\n",
    "bi_lstm_model = tf.keras.Model(inputs, outputs, name=\"bidirectional_model\")\n",
    "\n",
    "# compile & fit\n",
    "\n",
    "\n",
    "\n",
    "# 4. CONV1D MODEL\n",
    "\n",
    "# To test out embedding layer, Conv1D and max pooling...\n",
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"]))\n",
    "conv_1d = layers.Conv1D(filters=32,\n",
    "                        kernel_size=5, # ngram of 5 (looks at 5 words at a time)\n",
    "                        activation=\"relu\",\n",
    "                        padding=\"valid\") # output is smaller than input shape\n",
    "conv_1d_output = conv_1d(embedding_test)\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output) # equivalent to get the most important feature\n",
    "\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape\n",
    "\n",
    "# Build conv1d model with Funcitonal API\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "\n",
    "x = layers.Conv1D(filters=64,\n",
    "                  kernel_size=5,\n",
    "                  activation=\"relu\",\n",
    "                  padding=\"valid\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "conv1d_model = tf.keras.Model(inputs, outputs, name=\"conv1d_model\")\n",
    "\n",
    "\n",
    "\n",
    "# 5. TRANSFER LEARNING MODEL (UNIVERSAL SENTENCE ENCODER)\n",
    "\n",
    "# Create a Keras layer using the USE prtrained layer from TF Hub\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[],\n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=False,\n",
    "                                        name=\"USE\")\n",
    "\n",
    "# Create USE model using sequential API\n",
    "use__tl_model = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\"),\n",
    "], name=\"USE_TL_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f508c",
   "metadata": {},
   "source": [
    "#### *Deploy the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650f896",
   "metadata": {},
   "source": [
    "(a) Save the trained model (e.g., using tf.keras.Model.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your best performing model (HDF5 format)\n",
    "best_model.save(\"best_model.h5\")\n",
    "\n",
    "# Alternatively, SavedModel format...\n",
    "best_model.save(\"best_model_SavedModel_format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2c149",
   "metadata": {},
   "source": [
    "(b) Load the saved model and make predictions (e.g., using tf.keras.models.load_model and **tf.keras.Model.predict**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with custom Hub Layer (required HDF5 format)\n",
    "import tensorflow_hub as hub\n",
    "loaded_best_model = tf.keras.models.load_model(\"best_model.h5\",\n",
    "                                            custom_objects={\"KerasLayer\": hub.KerasLayer})\n",
    "\n",
    "# How does the loaded model perform? (Confirm its the same model)\n",
    "loaded_best_model.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e3a57",
   "metadata": {},
   "source": [
    "#### *Improvements*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15533769",
   "metadata": {},
   "source": [
    "(a) *Consider how you want your data to look at and which format to aim for:*\n",
    "\n",
    "* It's important to consider the format and structure of the data, especially if you have unstructured text data. You may want to preprocess the data by removing stop words, special characters, or perform stemming or lemmatization to reduce the vocabulary size and improve model performance. Additionally, you may want to consider encoding the data in different formats, such as one-hot encoding, **bag-of-words**, or **term frequency-inverse document frequency (TF-IDF)** to capture different aspects of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7088975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tf-idf\n",
    "    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e513d1",
   "metadata": {},
   "source": [
    "(b) *Create char-level tokenizer:*\n",
    "\n",
    "*  In addition to using a word-level tokenizer, you can also consider creating a **character-level tokenizer** to capture character-level information in the text. \n",
    "* This involves splitting sentences into individual characters, finding the average example char-length, and checking the distribution of character lengths to identify the range that covers around 95% of the sequences. You'll also need to create a list of all keyboard characters, create a character tokenizer instance, and test it on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac97729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))\n",
    "\n",
    "# Text splitting non-character-level sequence into characters\n",
    "split_chars(random_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# Split sequence-level data splits into char-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "\n",
    "\n",
    "\n",
    "# What's the average character length?\n",
    "char_len = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_len)\n",
    "mean_char_len\n",
    "\n",
    "\n",
    "\n",
    "# Check distribution of sequences at the character level\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(char_len, bins=7);\n",
    "\n",
    "\n",
    "\n",
    "# What character length covers 95% of sequences?\n",
    "output_seq_char_len = int(np.percentile(char_len, 95))\n",
    "output_seq_char_len\n",
    "\n",
    "\n",
    "\n",
    "# Get all keyboard characters with Python\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet\n",
    "\n",
    "\n",
    "\n",
    "# Create char-level token vectorizer instance\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and [OOV/UNK] tokens\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    name=\"char_vectorizer\")\n",
    "\n",
    "\n",
    "\n",
    "# Adapt character vectorizer to training chars\n",
    "char_vectorizer.adapt(train_chars)\n",
    "\n",
    "\n",
    "\n",
    "# Check character vocab stats\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
    "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab[-5:]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Test character vectorizer\n",
    "random_train_chars = random.choice(train_chars)\n",
    "print(f\"Charified text:\\n {random_train_chars}\")\n",
    "print(f\"\\nLength of random_train_chars: {len(random_train_chars.split())}\")\n",
    "\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "print(f\"\\nVectorized chars:\\n {vectorized_chars}\")\n",
    "print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e86fb8",
   "metadata": {},
   "source": [
    "(c) *Create char-level embedding layer:*\n",
    "\n",
    "* After creating a character-level tokenizer, you can create a **character-level embedding layer** to map each character to a vector representation. This can be useful in capturing morphology and spelling variations in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a character level embedding layer\n",
    "char_embed = layers.Embedding(input_dim=len(char_vocab),\n",
    "                                    output_dim=25,\n",
    "                                    mask_zero=True,\n",
    "                                    name=\"char_embed\")\n",
    "\n",
    "\n",
    "\n",
    "# Test character embedding layer\n",
    "print(f\"Charified text:\\n {random_train_chars}\\n\")\n",
    "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
    "print(f\"Embedded chars after vectorization and embedding:\\n {char_embed_example}\\n\")\n",
    "print(f\"Character embedding shape: {char_embed_example.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf8ae7",
   "metadata": {},
   "source": [
    "(d) *Consider a Conv1D model for this (**see examples above**):*\n",
    "\n",
    "* When working with character-level embeddings, you can consider using a **Conv1D** model instead of an **LSTM** or **GRU**. This is because a Conv1D model can learn to capture local patterns in the sequence, which may be more relevant for character-level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE CONV1D CHAR-LEVEL EMBEDDING MODEL\n",
    "\n",
    "# Build Conv1D model for character-level embeddings\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "vectorizer = char_vectorizer(inputs)\n",
    "embeddings = char_embed(vectorizer)\n",
    "\n",
    "x = layers.Conv1D(filters=64,\n",
    "                  kernel_size=5,\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\")(embeddings)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "outputs= layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "char_level_model = tf.keras.Model(inputs, outputs, name=\"char_level_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe5745",
   "metadata": {},
   "source": [
    "(e) *Combine token + char embedding layers with **layers.concatenate**:*\n",
    "\n",
    "* You can combine both the token-level and character-level embeddings by using the concatenate layer in Keras. This allows the model to capture both word-level and character-level information in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE TOKEN + CHAR EMBEDDINGS MODEL\n",
    "\n",
    "# 1. Setup token inputs/model\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_output)\n",
    "\n",
    "# 2. Setup char inputs/model\n",
    "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Concatenate token and char inputs (create hybrid token embedding)\n",
    "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, \n",
    "                                                                  char_model.output])\n",
    "\n",
    "# 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf\n",
    "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
    "combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers\n",
    "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
    "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
    "\n",
    "# 5. Construct model with char and token inputs\n",
    "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
    "                         outputs=output_layer,\n",
    "                         name=\"model_4_token_and_char_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393a754",
   "metadata": {},
   "source": [
    "(f) *Combine chars and tokens into a dataset using tf.data.Dataset (.from_tensor_slices) + Prefetch/batch:*\n",
    "\n",
    "* After combining the token-level and character-level embeddings, you can create a dataset using the **tf.data.Dataset.from_tensor_slices** method and preprocess it by **batching** and **prefetching** the data to improve training speed and memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char-level dataset\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148da179",
   "metadata": {},
   "source": [
    "(g) *Create positional embeddings:*\n",
    "\n",
    "* To capture the positional information of the text, you can create **positional embeddings** that represent the order of the words or characters in the sequence. This involves checking the distribution of the \"line_number\" column, creating one-hot tensors of the \"line_number\" column, and checking the distribution and coverage of the \"total_lines\" at different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ...\n",
    "\n",
    "# Determine the maximum value for line_number\n",
    "max_line_number = max(dataset['line_number'])\n",
    "\n",
    "# Create one-hot tensors for line_number\n",
    "line_number_onehot = tf.one_hot(dataset['line_number'] - 1, depth=max_line_number)\n",
    "\n",
    "# Determine the maximum value for total_lines\n",
    "max_total_lines = max(dataset['total_lines'])\n",
    "\n",
    "# Create one-hot tensors for total_lines\n",
    "total_lines_onehot = tf.one_hot(dataset['total_lines'] - 1, depth=max_total_lines)\n",
    "\n",
    "# Concatenate the one-hot tensors\n",
    "positional_embedding = tf.concat([line_number_onehot, total_lines_onehot], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d89111",
   "metadata": {},
   "source": [
    "(h) *Apply label smoothing:*\n",
    "\n",
    "* **Label smoothing** is a regularization technique that involves replacing the hard label targets with a smoothed distribution. This can improve the generalization of the model and prevent overfitting, especially when working with small datasets or highly imbalanced classes.\n",
    "\n",
    "***(Visit: https://www.pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/ for more info.)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def label_smoothing_loss(y_true, y_pred, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss with label smoothing.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels, tensor of shape (batch_size, num_classes).\n",
    "        y_pred: Predicted labels, tensor of shape (batch_size, num_classes).\n",
    "        epsilon: Smoothing factor, float between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        The label-smoothed cross-entropy loss.\n",
    "    \"\"\"\n",
    "    num_classes = y_pred.shape[-1]\n",
    "    y_true_smooth = (1 - epsilon) * y_true + epsilon / num_classes\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true_smooth, y_pred)\n",
    "\n",
    "# Example usage\n",
    "model.compile(optimizer='adam', loss=label_smoothing_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f2e44",
   "metadata": {},
   "source": [
    "## *(7) Time Series - Compiled Process*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf8ed3",
   "metadata": {},
   "source": [
    "#### *Explore the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431dfcb",
   "metadata": {},
   "source": [
    "This step includes not only checking the number of samples and formatting, but also understanding the **time range**, **frequency**, and any missing values or anomalies in the data. You may need to perform data cleaning, **imputation**, or normalization to prepare the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "df = pf.read_csv('/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv',\n",
    "                parse_dates=[\"Date\"],\n",
    "                index_col=[\"Date\"]) # parse date column as datetime\n",
    "\n",
    "df[\"Price (Example)\"].plot(kind='line')\n",
    "\n",
    "prices = pd.DataFrame(df[\"Price (Example)\"]).rename(columns={\"Price (Example)\": \"Price\"})\n",
    "\n",
    "\n",
    "\n",
    "# Importing time series data with Python's CSV module\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "timesteps = []\n",
    "prices = []\n",
    "with open(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \"r\") as f:\n",
    "  csv_reader = csv.reader(f, delimiter=\",\")\n",
    "  next(csv_reader) # skip first line (titles)\n",
    "  for line in csv_reader:\n",
    "    timesteps.append(datetime.strptime(line[1], \"%Y-%m-%d\")) # get dates from strs\n",
    "    prices.append(float(line[2])) # get closing price as a float\n",
    "\n",
    "\n",
    "# Visualize more clearly with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "prices.plot(figsize=(10, 7))\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Price of Item from XX/XX/XX to XX/XX/XX\", fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the data the RIGHT WAY\n",
    "split_size = int(0.8.len(prices)) # 80% train, 20% test\n",
    "\n",
    "# Create train data splits\n",
    "X_train, y_train = timesteps[:split_size], prices[:split_size]\n",
    "\n",
    "X_test, y_test = timesteps[split_size:], prices[split_size:]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "\n",
    "\n",
    "# To make dataset more performant --> tf.data API\n",
    "train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "\n",
    "test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "\n",
    "# Combine labels and features by zipping together -> (features, labels)\n",
    "train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
    "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n",
    "\n",
    "# Batch and prefetch\n",
    "BATCH_SIZE = 1024\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709efe6",
   "metadata": {},
   "source": [
    "#### *Create windowed dataset(s)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624b6ad",
   "metadata": {},
   "source": [
    "This involves creating sliding windows of fixed length and step size from the time series data. The **window size** and **step size** should be carefully chosen based on the nature of the problem and the time series characteristics, such as the **seasonality**, trend, and noise. You may also want to add additional features to the windowed data, such as **lagged values**, **moving averages**, or **Fourier transforms**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab01393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup global variables for WINDOW and HORIZON\n",
    "HORIZON = 1\n",
    "WINDOW = 7\n",
    "\n",
    "# Functionize windowing of labels\n",
    "def get_labelled_windows(x, horizon=HORIZON):\n",
    "    \"\"\"\n",
    "    Creates labels for windowed dataset.\n",
    "\n",
    "      E.g., if horizon = 1\n",
    "      Input: [0, 1, 2, 3, 4, 5, 6, 7] -> Output: ([0, 1, 2, 3, 4, 5, 6], [7])\n",
    "    \"\"\"\n",
    "    \n",
    "    return x[:, :-horizon], x[:, -horizon:]\n",
    "\n",
    "# Test out the function\n",
    "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0))\n",
    "print(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create function to view NumPy arrays as windows\n",
    "import numpy as np\n",
    "\n",
    "def make_windows(x, window_size=WINDOW_SIZE, horizon=HORIZON):\n",
    "  \"\"\" Turns a 1D array into a 2D array of sequential labelled windows of\n",
    "  window_size with horizon size labels.\n",
    "  \"\"\"\n",
    "  # 1. Create a window of specific window_size (add the horizon on the end for labelling later)\n",
    "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
    "\n",
    "  # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
    "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size + horizon-1)), axis=0).T\n",
    "  print(f\"Window indexes:\\n {window_indexes, window_indexes.shape}\")\n",
    "\n",
    "  # 3. Use array produced array to index on a target array (the time series)\n",
    "  windowed_array = x[window_indexes]\n",
    "\n",
    "  # 4. Get the labelled windows (functionized above)\n",
    "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
    "  return windows, labels\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "full_windows, full_labels = make_windows(prices, WINDOW, HORIZON)\n",
    "len(full_windows), len(full_labels)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Window: {full_windows[i]} -> Label {full_label[i]})\n",
    "          \n",
    "          \n",
    "          \n",
    "# Functionize creation of train and test splits\n",
    "def make_train_test_splits(windows, labels, test_split=0.2):\n",
    "  \"\"\"\n",
    "  Splits matching pairs of windows and labels into train and test splits.\n",
    "  \"\"\"\n",
    "  \n",
    "  split_size = int(len(windows) * (1-test_split)) # default to an 80-20 tr-ts split\n",
    "  train_windows = windows[:split_size]\n",
    "  train_labels = labels[:split_size]\n",
    "  test_windows = windows[split_size:]\n",
    "  test_labels = labels[split_size:]\n",
    "\n",
    "  return train_windows, test_windows, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8593c",
   "metadata": {},
   "source": [
    "#### *Consider various potential models*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241aa60",
   "metadata": {},
   "source": [
    "Time series problems can be approached with a variety of models, including statistical models, machine learning models, and deep learning models. Some popular models for time series forecasting include **ARIMA**, **SARIMA**, **Prophet**, LSTM, and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.SIMPLE DENSE MODEL (WINDOW = 7, HORIZON = 1 <-- change as necessary)\n",
    "dense_model = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(HORIZON, activation=\"linear\")\n",
    "], name=\"dense_model\")\n",
    "\n",
    "dense_model.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "dense_model_history = dense_model.fit(train_windows, train_labels, epochs=100,\n",
    "                              batch_size=128,\n",
    "                              validation_data=(test_windows, test_labels),\n",
    "                              verbose=True,\n",
    "                              callbacks=[create_model_checkpoint(model_name=model_4.name)])\n",
    "\n",
    "\n",
    "\n",
    "# 2. CONV1D MODEL\n",
    "expand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))\n",
    "\n",
    "conv_model = tf.keras.Sequential([\n",
    "    expand_dims_layer,\n",
    "    layers.Conv1D(filters = 128,\n",
    "                 kernel_size = 5,\n",
    "                 padding='causal',\n",
    "                 activation='relu'),\n",
    "    layers.Dense(HORIZON)\n",
    "], name = 'conv1d_model')\n",
    "\n",
    "\n",
    "\n",
    "# 3. LSTM MODEL\n",
    "inputs = layers.Input(shape=(WINDOW))\n",
    "\n",
    "x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs)\n",
    "\n",
    "#x = layers.LSTM(128, return_sequences=True)(x)\n",
    "x = layers.LSTM(128, activation=\"relu\")(x)\n",
    "#x = layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "outputs = layers.Dense(HORIZON)(x)\n",
    "lstm_model = tf.keras.Model(inputs, outputs, name=\"lstm_model\")\n",
    "\n",
    "\n",
    "\n",
    "# 4. BIDIRECTIONAL LSTM MODEL\n",
    "bidir_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(64, kernel_size = 3,\n",
    "                          strides = 1, padding = 'causal',\n",
    "                          activation = 'relu',\n",
    "                          input_shape = [WINDOW, HORIZON]),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, return_sequences = True)),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(WINDOW, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 5. ENSEMBLE MODEL (SEE NON-PROBLEM-SPECIFIC TASKS SECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbc75f",
   "metadata": {},
   "source": [
    "#### *Prepare metrics + eval-pipeline function*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7d686",
   "metadata": {},
   "source": [
    "The evaluation metrics for time series models may differ from those used in other types of problems. For example, in forecasting problems, you may want to use metrics such as ***mean absolute error (MAE)**, **mean squared error (MSE)**, or **symmetric mean absolute percentage error (SMAPE)**. You may also need to define custom loss functions or performance measures based on the specific problem requirements. Additionally, it is important to establish an evaluation pipeline that allows you to compare and select the best model based on the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73465820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASE implementation\n",
    "def mean_absolute_scaled_error(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Implement MASE (assuming no seasonality of a given dataset).\n",
    "  \"\"\"\n",
    "  mae = tf.reduce_mean(tf.abs(y_true-y_pred))\n",
    "\n",
    "  # Find MAE of naive forecast (no seasonality)\n",
    "  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:]-y_true[:-1]))\n",
    "\n",
    "  return mae / mae_naive_no_season\n",
    "\n",
    "\n",
    "\n",
    "# Prediction evaluation function\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "  # Make sure float32 (for metric calculations)\n",
    "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "  # Calculate various metrics\n",
    "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n",
    "  rmse = tf.sqrt(mse)\n",
    "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "  \n",
    "  return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy(),\n",
    "          \"mase\": mase.numpy()}\n",
    "\n",
    "\n",
    "\n",
    "# Adjust evaluate_preds() fn to work for larger horizons\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "  # Make sure float32 (for metric calculations)\n",
    "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "  # Calculate various metrics\n",
    "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n",
    "  rmse = tf.sqrt(mse)\n",
    "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "  \n",
    "  # Account for different sized metrics (for longer horizons, reduce metrics to single value)\n",
    "  if mae.ndim > 0:\n",
    "    mae = tf.reduce_mean(mae)\n",
    "    mse = tf.reduce_mean(mse)\n",
    "    rmse = tf.reduce_mean(rmse)\n",
    "    mape = tf.reduce_mean(mape)\n",
    "    mase = tf.reduce_mean(mase)\n",
    "\n",
    "  return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy(),\n",
    "          \"mase\": mase.numpy()}\n",
    "\n",
    "\n",
    "\n",
    "# To compute a moving average on windowed time series data...\n",
    "def moving_average_forecast(series, window_size):\n",
    "    \"\"\"\n",
    "    Forecasts the mean of the last few values.\n",
    "    \n",
    "    If window_size = 1, then this is equivalent to a naive forecast\n",
    "    \"\"\"\n",
    "    \n",
    "    forecast = []\n",
    "    \n",
    "    for time in range(len(series) - window_size):\n",
    "        forecast.append(series[time:time + window_size].mean())\n",
    "        \n",
    "    # Convert to NP array\n",
    "    np_forecast = np.array(forecast)\n",
    "    \n",
    "    return np_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108860d3",
   "metadata": {},
   "source": [
    "#### *Define desired callbacks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77528b6c",
   "metadata": {},
   "source": [
    " Callbacks are functions that can be called at specific points during training to perform certain actions, such as saving model checkpoints, logging training progress, or early stopping. Some useful callbacks for time series modeling include ModelCheckpoint, EarlyStopping, TensorBoard, and **ReduceLROnPlateau**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b39cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create modelling checkpoint to save the best model during training\n",
    "import os\n",
    "\n",
    "# Create a function to implement a ModelCheckpoint cb with a specific filename\n",
    "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
    "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path,\n",
    "                                                                  model_name),\n",
    "                                            monitor=\"val_loss\",\n",
    "                                            verbose=0, # limit output\n",
    "                                            save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "# ReduceLROnPlateau callback\n",
    "tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                    patience=100,\n",
    "                                    verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "# EarlyStopping callbacks\n",
    "tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                patience=25,\n",
    "                                restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9438c6",
   "metadata": {},
   "source": [
    "#### *Identify an optimal Learning Rate*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9080578",
   "metadata": {},
   "source": [
    " Learning rate is a hyperparameter that determines the step size of gradient descent during training. Choosing an appropriate learning rate can significantly affect the model performance and training time. You can try different learning rates using techniques such as **grid search** or **random search**, or use adaptive learning rate methods such as Adam or RMSprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(dataset):\n",
    "    \n",
    "    model = create_uncompiled_model() # choose and run a model\n",
    "    \n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-4 * 10**(epoch / 20))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    model.compile(loss = tf.keras.losses.Huber(),\n",
    "                 optimizer = opt,\n",
    "                 metrics = ['mae'])\n",
    "    \n",
    "    history = model.fit(dataset, epochs = 100, callbacks = [lr_scheduler])\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run training with dynamic LR\n",
    "lr_history = adjust_learning_rate(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860ee19",
   "metadata": {},
   "source": [
    "#### *Make + plot forecasts*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7455f",
   "metadata": {},
   "source": [
    "After training the model, you can use it to make forecasts for future time steps. You may also want to visualize the forecasts and compare them with the actual data to assess the model accuracy and usefulness. Plotting the model residuals and error metrics can also help you diagnose any model deficiencies or anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many timesteps into the future to predict\n",
    "INTO_FUTURE = 14\n",
    "\n",
    "\n",
    "\n",
    "# 1. Create function to make predictions into the future\n",
    "def make_future_forecasts(values, model, into_future, window_size=WINDOW_SIZE) -> list:\n",
    "  \"\"\"\n",
    "  Make future forecasts into future steps after values ends.\n",
    "\n",
    "  Returns future forecasts as a list of floats.\n",
    "  \"\"\"\n",
    "  # 2. Create an empty list for future forecasts/prepare data to forecast on\n",
    "  future_forecast = []\n",
    "  last_window = values[-WINDOW_SIZE:]\n",
    "\n",
    "  # 3. Make INTO_FUTURE number of predictions, altering the data thats predicted on each time\n",
    "  for _ in range(INTO_FUTURE):\n",
    "    # Predict on the last window then append it again, again, again...\n",
    "    # (our model will eventually begin to make forecasts on its own forecasts)\n",
    "    future_pred = model.predict(tf.expand_dims(last_window, axis=0))\n",
    "    print(f\"Predicting on:\\n {last_window} -> Prediction: {tf.squeeze(future_pred).numpy()}\\n\")\n",
    "\n",
    "    # Append prediction to future_forecast\n",
    "    future_forecast.append(tf.squeeze(future_pred).numpy())\n",
    "\n",
    "    # Update last window with new pred and get WINDOW_SIZE most recent preds\n",
    "    # (model was trained on WINDOW_SIZE windows)\n",
    "    last_window = np.append(last_window, future_pred)[-WINDOW_SIZE:]\n",
    "\n",
    "  return future_forecast\n",
    "\n",
    "\n",
    "\n",
    "# Make forecasts into the future\n",
    "future_forecast = make_future_forecasts(y_all,\n",
    "                                        model_9,\n",
    "                                        INTO_FUTURE,\n",
    "                                        WINDOW_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "def get_future_dates(start_date, into_future, offset=1):\n",
    "  \"\"\"\n",
    "  Returns array of datetime values ranging from start_date to start_date+into_future.\n",
    "  \"\"\"\n",
    "  start_date = start_date + np.timedelta64(offset, \"D\") # specify start date\n",
    "  end_date = start_date + np.timedelta64(into_future, \"D\") # specify end date\n",
    "  return np.arange(start_date, end_date, dtype=\"datetime64[D]\")\n",
    "\n",
    "# Last timestep of timesteps(currently in np.datetime64 format)\n",
    "last_timestep = prices.index[-1]\n",
    "\n",
    "# Get next two weeks of timesteps\n",
    "next_time_steps = get_future_dates(start_date=last_timestep,\n",
    "                                  into_future=INTO_FUTURE)\n",
    "\n",
    "# Insert last timestep/final price into next timestep and future forecasts\n",
    "# to prevent disjointed graph\n",
    "next_time_steps = np.insert(next_time_steps, 0, last_timestep)\n",
    "future_forecast = np.insert(future_forecast, 0, btc_price[-1])\n",
    "\n",
    "# Plot future price predictions\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_time_series(prices.index,\n",
    "                 price,\n",
    "                 start=1500,\n",
    "                 format=\"-\",\n",
    "                 label=\"Actual Price\")\n",
    "plot_time_series(next_time_steps,\n",
    "                 future_forecast, \n",
    "                 format=\"-\",\n",
    "                 label=\"Predicted Price\")\n",
    "\n",
    "\n",
    "\n",
    "# ALTERNATIVE PLOTTING FUNCTION\n",
    "def plot_series(time, series, format = \"-\", title = \"\", label = None, start = 0, end = None):\n",
    "    \"\"\"\n",
    "    Plot the series.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.plot(time[started:end], series[start:end], format, label=label)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(title)\n",
    "    if label:\n",
    "        plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d766659",
   "metadata": {},
   "source": [
    "#### *Save + export model (.h5)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5c837",
   "metadata": {},
   "source": [
    "Once you have selected the best model, you should save it to disk for future use or deployment. The Keras API provides a convenient way to save and load models in the HDF5 format, which can be easily integrated into other applications or frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518060ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_time_series_model/my_model')\n",
    "\n",
    "# To compress the directory using tar...\n",
    "! tar -czvf saved_model.tar.gz save_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56469c",
   "metadata": {},
   "source": [
    "# Non-Problem-Specific Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03304b60",
   "metadata": {},
   "source": [
    "#### *Save models in various formats*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b8e1e",
   "metadata": {},
   "source": [
    "* SavedModel format:\n",
    "\n",
    "```\n",
    "model.save(\"saved_trained_model_name\")\n",
    "\n",
    "```\n",
    "\n",
    "* HDF5 format:\n",
    "\n",
    "```\n",
    "model.save('path/to/model.h5')\n",
    "```\n",
    "\n",
    "* Testing a loaded mdoel:\n",
    "\n",
    "```\n",
    "loaded_model = tf.keras.models.load_model(\"saved_trained_model_name\")\n",
    "loaded_model.evaluate(test_data)\n",
    "\n",
    "model.evaluate(test_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c6490",
   "metadata": {},
   "source": [
    "#### *Plotting loss & accuracy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "741be7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(history):\n",
    "    \"\"\"\n",
    "    Returns separate loss curves for the training and validation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = baseline_hist.history[\"loss\"]\n",
    "    val_loss = baseline_hist.history[\"val_loss\"]\n",
    "    \n",
    "    accuracy = baseline_hist.history[\"accuracy\"]\n",
    "    val_accuracy = baseline_hist.history[\"val_accuracy\"]\n",
    "    \n",
    "    epochs = range(len(baseline_hist.history[\"loss\"]))\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.plot(epochs, loss, label=\"training_loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"val_loss\")\n",
    "    plt.title(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label=\"training_accuracy\")\n",
    "    plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n",
    "    plt.title(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea364239",
   "metadata": {},
   "source": [
    "#### *Preventing overfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90a7f1",
   "metadata": {},
   "source": [
    "Some things to try, if you are running into problems with a model:\n",
    "* Adding model layers (e.g., `Conv2D`, `MaxPool2D`, `Dense`, etc.)\n",
    "* Increase the number of filters in each Conv layer\n",
    "* Increase the number of hidden units\n",
    "* Try implementing Dropout\n",
    "* Add data augmentation\n",
    "* Change activation functions (and ensure you've selected an appropriate one to begin with)\n",
    "* Change the optimization function\n",
    "* Optimize the learning rate (see below)\n",
    "* Fit the model on more data\n",
    "* Fit the model for longer\n",
    "* Implement various callbacks\n",
    "* Use ***transfer learning*** to leverage what another model has learned and use it for your own use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45b2b5",
   "metadata": {},
   "source": [
    "#### *Data augmentation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b37f",
   "metadata": {},
   "source": [
    "* Create an ImageDataGenerator instance with data augmentation\n",
    "```\n",
    "train_datagen_aug = ImageDataGenerator(rescale=1/255.,\n",
    "                                      rotation_range=0.2,\n",
    "                                      shear_range=0.2,\n",
    "                                      zoom_range=0.2,\n",
    "                                      width_shift_range=0.2,\n",
    "                                      height_shift_range=0.2,\n",
    "                                      horizontal_flip=True)\n",
    "```\n",
    "\n",
    "* Import data and augment it from training directory\n",
    "```\n",
    "train_datagen_aug = train_datagen_aug.flow_from_directory(train_dir,\n",
    "                                                         target_size=(224, 224),\n",
    "                                                         class_mode=\"binary\",\n",
    "                                                         shuffle=True)\n",
    "```\n",
    "\n",
    "Be sure to select appropriate values for each argument, especially your image `target_size` and `class_mode` (could be `categorical` instead of `binary`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d49833",
   "metadata": {},
   "source": [
    "#### *Dropout*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aede6b",
   "metadata": {},
   "source": [
    "* Define a NN architecture:\n",
    "```\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    ")]\n",
    "```\n",
    "\n",
    "* Add a Dropout layer:\n",
    "```\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),   <----- Specify dropout rate %\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    ")]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a4ba3",
   "metadata": {},
   "source": [
    "#### *Batch loading of data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cffed",
   "metadata": {},
   "source": [
    "* Map preprocessing function to training data (and parallelize):\n",
    "```\n",
    "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "* Shuffle train_data and turn it into batches and prefetch it (load it faster):\n",
    "```\n",
    "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "* Map preprocessing function to test data:\n",
    "```\n",
    "test_data = test_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "* Turn test data into batches (don't need to shuffle)\n",
    "```\n",
    "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "*(See pg 422 of Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow Book by Aurelion Geron)* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebcb7c",
   "metadata": {},
   "source": [
    "#### *Callbacks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28ad84",
   "metadata": {},
   "source": [
    "* **ModelCheckpoint**:\n",
    "```\n",
    "filepath = \"path/to/checkpoints\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                         save_best_only=True,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         monitor='val_loss,\n",
    "                                                         verbose=True)\n",
    "```\n",
    "\n",
    "* **EarlyStopping**:\n",
    "```\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                       patience=5,\n",
    "                                                       verbose=1)\n",
    "```\n",
    "\n",
    "* **TensorBoard**:\n",
    "```\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='directory_name/logs',\n",
    "                                                       histogram_freq=1,\n",
    "                                                       profile_batch=0)\n",
    "```\n",
    "\n",
    "* **LearningRateScheduler**:\n",
    "```\n",
    "learningrate_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "```\n",
    "\n",
    "* **ReduceLROnPlateau**:\n",
    "```\n",
    "reducelr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                         factor=0.1,\n",
    "                                                         patience=3,\n",
    "                                                         verbose=1)\n",
    "```\n",
    "\n",
    "* *Implement desired callbacks as follows*:\n",
    "```\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[checkpoint_callback,\n",
    "                                earlystop_callback,\n",
    "                                tensorboard_callback,\n",
    "                                learningrate_callback,\n",
    "                                reducelr_callback])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde36d3e",
   "metadata": {},
   "source": [
    "#### *Dataset formats (JSON, CSV, pandas)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10f145",
   "metadata": {},
   "source": [
    "* **JSON**:\n",
    "\n",
    "```\n",
    "import json\n",
    "import os\n",
    "\n",
    "!wget github.content/url\n",
    "\n",
    "with open(\"file.json\", \"r\") as f:\n",
    "    example = json.load(f)\n",
    "    \n",
    "print(example)\n",
    "```\n",
    "\n",
    "* **CSV**:\n",
    "\n",
    "```\n",
    "import csv\n",
    "\n",
    "with open('data.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        [ YOUR CODE HERE ]\n",
    "        \n",
    "```\n",
    "\n",
    "* **pandas**:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09cf73",
   "metadata": {},
   "source": [
    "#### *Dynamically adjusting learning rates*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05ce25",
   "metadata": {},
   "source": [
    "* Find ideal learning rate:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lrs = 1e-3 * 10**(tf.range(40)/20)\n",
    "plt.semilogx(lrs, fit_lr_history.history[\"loss\"])\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Find the ideal LR\")\n",
    "```\n",
    "\n",
    "* Schedule learning rate for future training:\n",
    "\n",
    "```\n",
    "learningrate_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feddfb4a",
   "metadata": {},
   "source": [
    "#### *Visualization of various classification + other metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc732172",
   "metadata": {},
   "source": [
    "* **Loss & Accuracy**:\n",
    "\n",
    "```\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set: {(accuracy*100):.2f}%\")\n",
    "```\n",
    "\n",
    "* **Precision, Recall, F1 score**\n",
    "\n",
    "```\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "```\n",
    "\n",
    "* **Confusion matrix**:\n",
    "\n",
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true = test_labels, y_pred = y_preds)\n",
    "```\n",
    "\n",
    "* **Create a more visual confusion matrix**:\n",
    "\n",
    "(See `make_confusion_matrix` function below)\n",
    "\n",
    "```\n",
    "make_confusion_matrix(y_true=test_labels,\n",
    "                        y_pred=y_preds,\n",
    "                        classes=class_names,\n",
    "                        figsize=(25, 25),\n",
    "                        text_size=15)\n",
    "```\n",
    "\n",
    "* **OR, Functionize an Evaluation Pipeline**:\n",
    "\n",
    "```\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def get_scores(y_true, y_pred):\n",
    "\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                   \"precision\": model_precision,\n",
    "                   \"recall\": model_recall,\n",
    "                   \"f1\": model_f1}\n",
    "  return model_results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7580900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15):\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_nrom = cm.astype(\"float\")/cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    n_classes = cm.shape[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    \n",
    "    cax = ax.matshow(cm, cmap = plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    # Set labels to be classes\n",
    "    if classes:\n",
    "        labels = classes\n",
    "    else:\n",
    "        labels = np.arange(cm.shape[0])\n",
    "        \n",
    "    # Label the axes\n",
    "    ax.set(title = \"Confusion Matrix\",\n",
    "          xlabel = \"Predicted label\",\n",
    "          ylabel = \"True label\",\n",
    "          xticks = np.arange(n_classes),\n",
    "          yticks = np.arange(n_classes),\n",
    "          xticklabels = labels,\n",
    "          yticklabels = labels)\n",
    "    \n",
    "    # Set x-axis labels to bottom\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis\n",
    "    \n",
    "    # Adjust label size\n",
    "    ax.yaxis.label.set_size(text_size)\n",
    "    ax.xaxis.label.set_size(text_size)\n",
    "    ax.title.set_size(text_size)\n",
    "    \n",
    "    # Set threshold for different colors\n",
    "    threshold = (cm.max() + cm.min()) / 2\n",
    "    \n",
    "    # Plot the text on the cell\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "                  horizontalalignment = \"center\",\n",
    "                  color = \"white\" if cm[i, j] > threshold else \"black\",\n",
    "                  size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7eea3e",
   "metadata": {},
   "source": [
    "#### *Making a dataset performant using tf.data API*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13815ff2",
   "metadata": {},
   "source": [
    "```\n",
    "train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "\n",
    "test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
    "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset, test_dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdff91",
   "metadata": {},
   "source": [
    "#### *Creating ensemble methods*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd73cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 7 # specific to your problem\n",
    "train_dataset = [1, 2, 3, 4, 5] # your traing data\n",
    "test_dataset = [6, 7, 8, 9, 10] # your test data\n",
    "\n",
    "def get_ensemble_models(horizon=HORIZON,\n",
    "                        train_data=train_dataset,\n",
    "                        test_data=test_dataset,\n",
    "                        num_iter=10,\n",
    "                        num_epochs=1000,\n",
    "                        loss_fns=[\"mae\", \"mse\", \"mape\"]):\n",
    "  \"\"\"\n",
    "  Returns a list of num_iter models, each trained on MAE, MSE and MAPE loss.\n",
    "\n",
    "  For example, if num_iter=10, a list of 30 trained models will be returned:\n",
    "  10 * len([\"mae\", \"mse\", \"mape\"]).\n",
    "  \"\"\"\n",
    "  # Make empty list for trained ensemble models\n",
    "  ensemble_models = []\n",
    "\n",
    "  # Create num_iter number of models per loss function\n",
    "  for i in range(num_iter):\n",
    "    # Build and fit a new model with a different loss function\n",
    "    for loss_function in loss_fns:\n",
    "      print(f\"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}\")\n",
    "\n",
    "      # Construct a simple model (similar to model_1)\n",
    "      model = tf.keras.Sequential([\n",
    "          layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),\n",
    "          layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),\n",
    "          layers.Dense(HORIZON)\n",
    "      ])\n",
    "\n",
    "      # Compile simple model with current loss_fn\n",
    "      model.compile(loss=loss_function,\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=[\"mae\", \"mse\"])\n",
    "      \n",
    "      # Fit the current model (i)\n",
    "      model.fit(train_data,\n",
    "                epochs=num_epochs,\n",
    "                verbose=0,\n",
    "                validation_data=test_data,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                            patience=200,\n",
    "                                                            restore_best_weights=True),\n",
    "                           tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                                patience=100,\n",
    "                                                                verbose=1)])\n",
    "      # Append fitted model to list of ensemble models\n",
    "      ensemble_models.append(model)\n",
    "\n",
    "  return ensemble_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910a695",
   "metadata": {},
   "source": [
    "#### *Accounting for horizon variance in time series prediction*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea5348",
   "metadata": {},
   "source": [
    "* (1) Use comparison operator to carry out logic depending on metric shape:\n",
    "* (2) Calculate various metrics\n",
    "* (3) Account for different sized metrics (for longer horizons, reduce metrics to single value)\n",
    "\n",
    "```\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "\n",
    "    if mae.ndim > 0:\n",
    "        mae = tf.reduce_mean(mae)\n",
    "        mse = tf.reduce_mean(mse)\n",
    "        rmse = tf.reduce_mean(rmse)\n",
    "        mape = tf.reduce_mean(mape)\n",
    "        mase = tf.reduce_mean(mase)\n",
    "\n",
    "    return {\"mae\": mae.numpy(),\n",
    "            \"mse\": mse.numpy(),\n",
    "            \"rmse\": rmse.numpy(),\n",
    "            \"mape\": mape.numpy(),\n",
    "            \"mase\": mase.numpy()}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a4f32",
   "metadata": {},
   "source": [
    "#### *Plotting prediction intervals*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab469878",
   "metadata": {},
   "source": [
    "`import numpy as np`\n",
    "\n",
    "* Get the median/mean values of our ensemble preds\n",
    "\n",
    "`ensemble_median = np.median(ensemble_preds, axis=0)`\n",
    "\n",
    "* Plot the median of our ensemble_preds along with the prediction intervals\n",
    "\n",
    "```\n",
    "offset = 500\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(X_test.index[offset:], y_test[offset:], \"r\", label=\"Test Data\")\n",
    "plt.plot(X_test.index[offset:], ensemble_median[offset:], \"-\", label=\"Ensemble Median\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"BTC Price\")\n",
    "plt.fill_between(X_test.index[offset:],\n",
    "                 (lower)[offset:],\n",
    "                 (upper)[offset:], label=\"Prediction Intervals\") # plot upper and lower bounds\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5805003",
   "metadata": {},
   "source": [
    "### *References*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6925e",
   "metadata": {},
   "source": [
    "# General Vocabulary and Terms List for Flashcard Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdbbd0",
   "metadata": {},
   "source": [
    "#### *Alphabetical vocabulary list:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb0646",
   "metadata": {},
   "source": [
    "* accuracy\n",
    "* activation functions\n",
    "* Adam optimization\n",
    "* ARIMA\n",
    "* bag-of-words\n",
    "* batching\n",
    "* batch normalization\n",
    "* batch size\n",
    "* binary cross entropy loss\n",
    "* categorical cross entropy loss\n",
    "* callbacks\n",
    "* character-level embedding layer\n",
    "* character-level tokenizers\n",
    "* checking the shapes\n",
    "* classification report\n",
    "* Confusion matrix\n",
    "* Conv1D models\n",
    "* Correlation matrix\n",
    "* data augmentation\n",
    "* data synthesis\n",
    "* early stopping callback\n",
    "* Embedding\n",
    "* exploring the data\n",
    "* f1 score\n",
    "* features\n",
    "* Fourier transforms\n",
    "* frequency\n",
    "* GRU\n",
    "* grid search\n",
    "* histogram\n",
    "* HDF5 format\n",
    "* imputation\n",
    "* KerasLayer\n",
    "* labels\n",
    "* label smoothing\n",
    "* lagged values\n",
    "* layer.concatenate\n",
    "* learning rate\n",
    "* learning rate decay function\n",
    "* learning rate scheduler\n",
    "* lemmatization\n",
    "* list of class_names\n",
    "* loss functions\n",
    "* LSTM\n",
    "* MaxPool2D\n",
    "* mean absolute error\n",
    "* mean squared error\n",
    "* metrics\n",
    "* missing values\n",
    "* model checkpoint callback\n",
    "* moving averages\n",
    "* normalization\n",
    "* one-hot encoding\n",
    "* optimizers\n",
    "* padding\n",
    "* plot the learning rate decay\n",
    "* positional embeddings\n",
    "* precision\n",
    "* prefetching\n",
    "* Prophet\n",
    "* random search\n",
    "* recall\n",
    "* ReduceLROnPlateau\n",
    "* regularization\n",
    "* removing outliers\n",
    "* ResNet\n",
    "* resizing your images\n",
    "* SavedModel format\n",
    "* SARIMA\n",
    "* scatter plot\n",
    "* seasonality\n",
    "* sigmoid activation\n",
    "* splitting the data\n",
    "* softmax activation\n",
    "* stemming\n",
    "* step size\n",
    "* stop words\n",
    "* symmetric mean absolute percentage error\n",
    "* TensorBoard callback\n",
    "* TensorFlow Hub\n",
    "* term frequency-inverse document frequency (TF-IDF)\n",
    "* tf.data.Dataset.from_tensor_slices\n",
    "* tf.keras.preprocessing.image_dataset_from_directory\n",
    "* tf.keras.layers.Embedding\n",
    "* tf.keras.Model\n",
    "* tf.keras.Model.predict\n",
    "* tf.keras.preprocessing.image.ImageDataGenerator\n",
    "* tf.keras.preprocessing.sequence.pad_sequences\n",
    "* tf.keras.preprocessing.text.one_hot\n",
    "* time range\n",
    "* tokens\n",
    "* tokenization\n",
    "* VGG16\n",
    "* visualizing the data\n",
    "* window size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21fe92",
   "metadata": {},
   "source": [
    "#### *Definitions/Descriptions*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3841506",
   "metadata": {},
   "source": [
    "* **accuracy** - Accuracy is a metric used to evaluate the performance of a classification model. It measures the percentage of correct predictions made by the model\n",
    "\n",
    "* **activation functions** - Activation functions are mathematical functions that are applied to the output of a neuron in a neural network. They introduce non-linearity into the network and help in modeling complex relationships between input and output variables. Common activation functions include `'sigmoid'`, `'relu'`, and `'tanh'`\n",
    "\n",
    "* **Adam optimization** - Adam optimization is an optimization algorithm used in neural networks. It combines the benefits of two other optimization algorithms, stochastic gradient descent and momentum, to improve the speed and efficiency of model training\n",
    "\n",
    "* **ARIMA** - ARIMA (*Autoregressive Integrated Moving Average*) is a statistical model used for time series analysis and forecasting. It is a popular model for modeling non-stationary time series data, and is characterized by three parameters: p, d, and q, which represent the order of the autoregressive, differencing, and moving average components, respectively\n",
    "\n",
    "* **bag-of-words** - The bag-of-words model is a simple and effective way of representing text data for machine learning models. In this model, a piece of text is represented as a bag (multiset) of its individual words, disregarding grammar and word order, but keeping track of the frequency of each word in the text. The bag-of-words model is often used as a feature extraction technique for text classification, sentiment analysis, and other natural language processing (NLP) tasks\n",
    "\n",
    "* **batching** - Batching is a technique used in machine learning to process data in batches or groups, rather than individually. Batching can improve the efficiency of model training by reducing the number of times the model needs to be updated based on individual data points. Batching is commonly used in NLP tasks to process sequences of text data of different lengths\n",
    "\n",
    "* **batch normalization** - a technique used in deep learning to normalize the inputs to a layer in a neural network by scaling and shifting them to a standard normal distribution. This technique helps in reducing the internal covariate shift, which is the change in the distribution of inputs to a layer during training, and can improve the model's performance\n",
    "\n",
    "* **batch size** - refers to the number of training examples used in each iteration of the training process. A larger batch size can lead to faster convergence, but it requires more memory and may lead to overfitting. A smaller batch size requires less memory, but it may converge more slowly\n",
    "\n",
    "* **binary cross entropy loss** - a type of loss function used in neural networks for binary classification problems. It measures the difference between the predicted probability of a positive class and the actual probability of the positive class\n",
    "\n",
    "* **categorical cross entropy loss** - a type of loss function used in neural networks for multi-class classification problems. It measures the difference between the predicted probability distribution and the actual probability distribution of the classes\n",
    "\n",
    "* **callbacks** - functions that can be called during the training of a machine learning model. They can be used to perform various tasks, such as saving the model, monitoring the training progress, or adjusting the learning rate\n",
    "\n",
    "* **character-level embedding layer** - A character-level embedding layer is a neural network layer that maps each character in a piece of text to a dense vector representation. This can be useful for NLP tasks where the meaning of a word is influenced by the specific characters it contains, such as misspellings, abbreviations, or slang. Character-level embedding layers can be used in conjunction with other neural network layers, such as Conv1D layers, to build machine learning models for text classification, sentiment analysis, and other tasks\n",
    "\n",
    "* **character-level tokenizers** - NLP tools that break down text into individual characters instead of words. This can be useful for tasks where the meaning of a word is not as important as its spelling or for languages with complex character systems. Character-level tokenizers can be used in conjunction with other NLP techniques, such as embedding layers, to build machine learning models for text classification, named entity recognition, and other tasks\n",
    "\n",
    "* **checking the shapes** - Checking the shapes refers to the process of verifying that the dimensions of the input and output tensors of a machine learning model are correct. This is important to ensure that the model is processing the data correctly and to prevent errors\n",
    "\n",
    "* **classification report** - A classification report is a summary of the performance of a classification model. It includes metrics such as accuracy, precision, recall, and F1 score, as well as the confusion matrix and other information about the model\n",
    "\n",
    "* **Confusion matrix** - A confusion matrix is a table used to evaluate the performance of a classification model. It shows the number of true positive, false positive, true negative, and false negative predictions made by the model\n",
    "\n",
    "* **Conv1D models** - Conv1D models are neural network models that use one-dimensional convolutional layers to process input data. In the context of NLP, Conv1D models are often used for text classification, sentiment analysis, and other tasks where the input data is a sequence of text. The convolutional layers can learn to recognize patterns in the text, such as sequences of words or characters, that are indicative of the task at hand. Conv1D models are often used in conjunction with other neural network layers, such as pooling layers or recurrent layers, to build more complex models for NLP tasks\n",
    "\n",
    "* **correlation matrix** - A correlation matrix is a table that shows the correlation coefficients between pairs of variables in a dataset. It is used to identify the strength and direction of the relationships between variables\n",
    "\n",
    "* **data augmentation** - Data augmentation refers to the process of generating new training data from existing data by applying transformations such as rotation, zooming, and flipping. This can help to increase the diversity and size of the dataset, which can improve the performance of a machine learning model\n",
    "\n",
    "* **data synthesis** - a technique used in machine learning to generate synthetic data that can be used to augment or supplement the existing data. Data synthesis is used to increase the size of the training data, improve the model's robustness, and reduce overfitting\n",
    "\n",
    "* **early stopping callback** - It is a technique used in machine learning to prevent overfitting of the model. During the training process, the model is evaluated on a validation set, and the training is stopped when the model starts to perform poorly on the validation set. The early stopping callback is a function that monitors the model's performance on the validation set and stops the training when the model's performance starts to degrade. This helps in preventing overfitting, and the model can generalize better on unseen data\n",
    "\n",
    "* **embedding** - a technique used in natural language processing to represent words or phrases as high-dimensional vectors in a mathematical space. Word embeddings are created by training a neural network on a large corpus of text data to learn the semantic relationships between words. Embeddings can be used to improve the accuracy of text classification, sentiment analysis, and information retrieval systems. They are also used in computer vision to represent images as high-dimensional vectors\n",
    "\n",
    "* **exploring the data** - Exploring the data involves examining the dataset to understand its properties, distributions, and patterns. This is an important step in machine learning as it helps in identifying the features that are relevant for the model, detecting outliers, and understanding the relationships between the variables\n",
    "\n",
    "* **f1 score** - F1 score is a metric used to evaluate the performance of a classification model. It is the harmonic mean of precision and recall, and it balances between the two metrics\n",
    "\n",
    "* **features** - Features are the measurable and observable properties or characteristics of an object or phenomenon that are used to build a machine learning model. In machine learning, features are the input variables that are fed into the algorithm to predict an output. For example, in a dataset of housing prices, features could include the number of bedrooms, square footage, location, etc.\n",
    "\n",
    "* **Fourier transforms** - A Fourier transform is a mathematical technique used to decompose a complex signal into its component frequencies. In machine learning and signal processing, Fourier transforms can be used to analyze and manipulate time series data, including speech recognition, image processing, and audio analysis\n",
    "\n",
    "* **frequency** -  Frequency refers to the rate at which something occurs, and can be an important factor in machine learning and AI. For example, if you are analyzing a dataset of user behavior on a website, you might look at the frequency with which users visit the site or perform certain actions, such as clicking on a particular button. Frequency can also refer to the frequency of a signal or waveform, such as in audio or image processing. In these cases, frequency analysis techniques such as Fourier transforms or wavelet transforms may be used to extract useful features from the data\n",
    "\n",
    "\n",
    "* **GRU** - GRU (Gated Recurrent Unit) is another type of recurrent neural network (RNN) architecture that is similar to LSTM but with fewer gates. GRUs use two gates (reset and update gates) to control the flow of information and better capture long-term dependencies in sequential data. Compared to LSTMs, GRUs are simpler and faster to train, but may not perform as well on more complex NLP tasks\n",
    "\n",
    "* **grid search** - Grid search is a hyperparameter tuning technique used to search for the optimal combination of hyperparameters for a machine learning model. It involves defining a grid of possible hyperparameter values and training and evaluating the model on all possible combinations of hyperparameters\n",
    "\n",
    "* **histogram** - A histogram is a graphical representation of the distribution of a dataset. It shows the frequency of data points that fall within a certain range of values. The x-axis represents the range of values, while the y-axis represents the frequency\n",
    "\n",
    "* **HDF5 format** - The `HDF5` format is file format used to save and load machine learning models. It is a hierarchical format that can store large amounts of data and metadata, making it useful for deep learning models\n",
    "\n",
    "* **imputation** - Imputation is a technique used to fill in missing values in a dataset. In machine learning, missing values can cause problems during training, as the model may not have enough information to make accurate predictions. Imputation involves filling in the missing values with estimates based on the remaining data\n",
    "\n",
    "* **KerasLayer** - a layer in TensorFlow that allows you to use pre-trained models from Keras or TensorFlow Hub as a layer in your own model. The KerasLayer is a feature in TensorFlow that enables you to use a pre-trained model as a layer in your own model with a simple API\n",
    "\n",
    "* **labels** - Labels are the output variables that machine learning algorithms try to predict based on the input variables or features. They represent the target or outcome of the model. For example, in a dataset of housing prices, the label could be the actual price of the house\n",
    "\n",
    "* **label smoothing** - Label smoothing is a regularization technique used in machine learning to prevent overfitting and improve generalization. It involves adding a small amount of noise to the ground truth labels during training. This helps to encourage the model to be less confident about its predictions and avoid becoming too focused on specific examples in the training data\n",
    "\n",
    "* **lagged values** - Lagged values refer to values of a time series that occurred at previous points in time. In time series analysis, lagged values can be used as features in a machine learning model to capture dependencies and correlations between past and present values. For example, if you are predicting the temperature tomorrow based on the temperature today, yesterday's temperature could be considered a lagged value. In TensorFlow, lagged values can be generated using functions such as `tf.keras.preprocessing.timeseries_dataset_from_array()`\n",
    "\n",
    "* **layer.concatenate** - The `layer.concatenate` function is a method in TensorFlow used for concatenating (i.e., joining) two or more tensors along a specified axis. This function is commonly used in building neural network architectures to combine the outputs of multiple layers or different branches of a network\n",
    "\n",
    "* **learning rate** - a hyperparameter that controls the step size during the optimization process. It determines how much the weights and biases of the model are adjusted during each iteration of the training process. A higher learning rate can lead to faster convergence, but it may also lead to instability and oscillation. A lower learning rate may take longer to converge, but it may be more stable and less likely to oscillate\n",
    "\n",
    "* **learning rate decay function** - A learning rate decay function is a function that reduces the learning rate of a machine learning model over time. It is often used to help the model converge more efficiently and effectively\n",
    "\n",
    "* **learning rate scheduler** - A learning rate scheduler is a function that adjusts the learning rate of a machine learning model during training. It can be used to improve the convergence of the model and prevent overfitting. Examples of learning rate schedulers include step decay, exponential decay, and cyclic learning rates\n",
    "\n",
    "* **lemmatization** - a process of reducing a word to its base or dictionary form, which is known as the lemma. Lemmatization is a more sophisticated technique than stemming, as it considers the context of the word and maintains its grammatical and semantic meaning. This technique is commonly used in natural language processing to improve the accuracy of text analysis and information extraction\n",
    "\n",
    "* **list of class_names** - A list of class_names is a list that contains the names of the different classes in a classification problem. It is often used in conjunction with a confusion matrix or classification report to label the rows and columns of the table\n",
    "\n",
    "* **loss functions** - Loss functions are used to evaluate the performance of a machine learning model. They measure the difference between the predicted output and the actual output. The goal of a loss function is to minimize the difference between the predicted and actual output. Examples of loss functions include mean squared error, categorical cross-entropy, and binary cross-entropy\n",
    "\n",
    "* **LSTM** - LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) architecture that is commonly used for natural language processing (NLP) tasks. LSTMs are designed to address the issue of vanishing gradients in traditional RNNs by using a memory cell and various gates (input, forget, and output gates) to selectively retain or discard information over time. This allows LSTMs to better capture long-term dependencies in sequential data, such as text\n",
    "\n",
    "* **MaxPool2D** - It is a type of pooling layer used in convolutional neural networks (CNNs). The MaxPool2D layer reduces the spatial dimensions of the input by taking the maximum value in each window of the input. This helps in reducing the number of parameters in the model and makes it less sensitive to small shifts and distortions in the input\n",
    "\n",
    "* **mean absolute error (MAE)** - MAE is a metric used to evaluate the accuracy of a machine learning model. It measures the average absolute difference between the predicted values and the true values. It is calculated by taking the average of the absolute differences between the predicted values and the true values\n",
    "\n",
    "* **mean squared error (MSE)** - MSE is a metric used to evaluate the accuracy of a machine learning model. It measures the average squared difference between the predicted values and the true values. It is calculated by taking the average of the squared differences between the predicted values and the true values\n",
    "\n",
    "* **metrics** - Metrics are used to measure the performance of a machine learning model. They provide a quantitative measure of how well the model is performing. Examples of metrics include accuracy, precision, recall, and F1 score\n",
    "\n",
    "* **missing values** - Missing values are data points that are not available in a dataset. They can occur due to a variety of reasons, such as human error, data corruption, or data loss. Missing values can be handled by either removing the data points, imputing the missing values with a suitable value, or using algorithms that can handle missing data\n",
    "\n",
    "* **model checkpoint callback** - It is a function used in machine learning to save the best model weights during the training process. The Model Checkpoint Callback is a feature in TensorFlow that allows you to save the best model weights based on a chosen metric, such as accuracy or loss. This helps in preventing the loss of progress in the training process and allows you to resume training from the last saved checkpoint\n",
    "\n",
    "* **moving averages** - A moving average is a technique used to smooth out fluctuations in a time series by calculating the average value of a subset of neighboring data points over a sliding window. The moving average can help to identify trends and patterns in the data, and can be useful in forecasting and anomaly detection\n",
    "\n",
    "* **normalization** - It is a technique used to preprocess the data before feeding it to a machine learning model. The idea is to scale the data so that it has zero mean and unit variance. This helps in improving the convergence of the model and makes it less sensitive to the scale of the input features. There are several types of normalization techniques, such as Z-score normalization, min-max normalization, and batch normalization\n",
    "\n",
    "* **one-hot encoding** - a technique used to represent categorical variables as binary vectors. Each category is represented as a vector of zeros and ones, where the position corresponding to the category is set to one and all other positions are set to zero. One-hot encoding is commonly used in machine learning algorithms that require numerical inputs, such as neural networks and decision trees\n",
    "\n",
    "* **optimizers** - Optimizers are algorithms used to optimize or adjust the weights and biases of a machine learning model during training. The goal of an optimizer is to minimize the loss function of the model by adjusting the weights and biases in the right direction. Examples of optimizers include Adam, RMSprop, and stochastic gradient descent (SGD)\n",
    "\n",
    "* **padding** - Padding refers to the process of adding additional elements to the edge of an input image or sequence to ensure that it is the desired size for a machine learning model. This can be useful when working with datasets that contain images or sequences of different sizes\n",
    "\n",
    "* **plot the learning rate decay** - Plotting the learning rate decay refers to the process of visualizing the learning rate decay function over time. This can help to identify any issues or anomalies with the function and ensure that it is working as intended\n",
    "\n",
    "* **positional embeddings** - Positional embeddings are a type of feature representation used in natural language processing (NLP) tasks, such as text classification or machine translation. They are used to encode the position of each word or token in a sequence, such as a sentence or paragraph, so that the model can better understand the context of each word\n",
    "\n",
    "* **precision** - Precision is a metric used to evaluate the performance of a classification model. It measures the percentage of true positive predictions out of all positive predictions made by the model\n",
    "\n",
    "* **prefetching** - Prefetching is a technique used in machine learning to improve data loading performance. It involves loading the data for the next batch while the current batch is being processed. This way, the data is ready for processing as soon as the current batch is finished, reducing the wait time for data loading and potentially increasing the overall speed of training. In TensorFlow, prefetching can be implemented using the `tf.data.Dataset.prefetch()` method, which allows you to specify the number of batches to prefetch\n",
    "\n",
    "* **Prophet** - Prophet is a forecasting model developed by Facebook that is designed to handle time series data with strong seasonal effects and multiple periods of seasonality. It is based on a generalized additive model (GAM) and uses Bayesian methods to estimate parameters and uncertainty intervals. Prophet has gained popularity for its ease of use and flexibility in handling a wide range of time series data\n",
    "\n",
    "* **random search** - Random search is a hyperparameter tuning technique used to search for the optimal combination of hyperparameters for a machine learning model. It involves randomly sampling hyperparameters from a defined distribution and training and evaluating the model on each sampled combination. Unlike grid search, it does not search over all possible combinations of hyperparameters, making it more computationally efficient\n",
    "\n",
    "* **recall** - Recall is a metric used to evaluate the performance of a classification model. It measures the percentage of true positive predictions out of all actual positive cases in the dataset\n",
    "\n",
    "* **ReduceLROnPlateau** - `ReduceLROnPlateau` is a callback function in TensorFlow that automatically adjusts the learning rate of a model when the validation loss has stopped improving. It reduces the learning rate by a factor specified by the user, allowing the model to continue to learn at a lower rate until it converges\n",
    "\n",
    "* **regularization** - It is a technique used to prevent overfitting in machine learning models. The idea is to add a penalty term to the loss function that the model is optimizing. The penalty term encourages the model to have smaller weights and biases, which can reduce overfitting. The two most commonly used regularization techniques are L1 and L2 regularization. L1 regularization adds a penalty term proportional to the absolute value of the weights, whereas L2 regularization adds a penalty term proportional to the square of the weights\n",
    "\n",
    "* **removing outliers** - Outliers are extreme values in a dataset that are significantly different from the rest of the values. They can affect the accuracy of machine learning models by skewing the data. Removing outliers involves identifying and eliminating data points that are far away from the mean or median of the dataset\n",
    "\n",
    "* **ResNet** - It is a deep convolutional neural network architecture that was developed by Microsoft Research Asia. The ResNet architecture introduced a new concept called residual connections, which allows the network to learn more easily and overcome the problem of vanishing gradients. ResNet is a popular choice for image classification, object detection, and segmentation tasks\n",
    "\n",
    "* **resizing your images** - Resizing your images refers to the process of changing the dimensions of an image. This is often done to standardize the size of the images in a dataset or to prepare them for use in a machine learning model\n",
    "\n",
    "* **SavedModel format** - The SavedModel format is a file format used to save and load machine learning models in TensorFlow. It is a portable format that can be used across different platforms and programming languages\n",
    "\n",
    "* **SARIMA** - SARIMA (Seasonal Autoregressive Integrated Moving Average) is a variant of the ARIMA model that includes additional parameters to account for seasonality in the data. In addition to the p, d, and q parameters, SARIMA models also include seasonal components represented by the P, D, and Q parameters, which capture the seasonal autoregressive, differencing, and moving average components, respectively\n",
    "\n",
    "* **scatter plot** - A scatter plot is a graphical representation of a dataset that displays the relationship between two variables. It is used to identify patterns, trends, and outliers in the data. The x-axis represents one variable, while the y-axis represents the other variable\n",
    "\n",
    "* **seasonality** - Seasonality refers to a repeating pattern or cycle in a time series. In time series analysis, seasonality can be caused by various factors such as the time of day, day of the week, or month of the year\n",
    "\n",
    "* **sigmoid activation** - Sigmoid activation is an activation function used in neural networks. It maps any input value to a value between 0 and 1, which makes it useful for binary classification problems. It is defined as `1 / (1 + exp(-x))`\n",
    "\n",
    "* **splitting the data** - Splitting the data refers to the process of dividing a dataset into training, validation, and test sets. The training set is used to train the machine learning model, the validation set is used to tune the hyperparameters of the model, and the test set is used to evaluate the performance of the model on unseen data\n",
    "\n",
    "* **softmax activation** - Softmax activation is a type of activation function used in neural networks. It maps the output of a model to a probability distribution over multiple classes. It is commonly used for multi-class classification problems\n",
    "\n",
    "* **stemming** - a process of reducing a word to its base or root form by removing the suffixes or prefixes. Stemming is a commonly used technique in natural language processing to simplify the text data and reduce its dimensionality. It can help to improve the accuracy of text classification and information retrieval systems\n",
    "\n",
    "* **step size** - Step size, also known as the stride, refers to the number of data points that the sliding window moves forward between each batch. The step size determines the degree of overlap between consecutive batches\n",
    "\n",
    "* **stop words** - a common term used in natural language processing (NLP) to refer to words that are filtered out from the text because they are considered to be of little importance or redundant. Stop words are typically very common words such as \"the\", \"a\", \"an\", \"in\", \"of\", etc. and can be removed from the text before processing it with machine learning algorithms. Removing stop words can help to reduce the dimensionality of the text data and improve the accuracy of NLP models\n",
    "\n",
    "* **symmetric mean absolute percentage error (SMAPE)** - SMAPE is a metric used to evaluate the accuracy of a machine learning model for time series data. It measures the percentage difference between the predicted values and the true values. Unlike MAE and MSE, SMAPE is symmetric, which means it gives equal weight to over-predictions and under-predictions\n",
    "\n",
    "* **TensorBoard callback** - The TensorBoard callback is a feature in TensorFlow that allows you to visualize and monitor the training of a machine learning model. It provides real-time feedback on the model's performance and allows you to monitor the loss and accuracy of the model during training\n",
    "\n",
    "* **TensorFlow Hub** - a library in TensorFlow that provides a central location for pre-trained machine learning models, including deep learning models, as well as modules and datasets. It allows you to easily use and transfer pre-trained models across different projects and applications\n",
    "\n",
    "* **term frequency-inverse document frequency (TF-IDF)** - TF-IDF is a commonly used technique for text feature extraction in NLP. It is used to measure the importance of a word in a document or corpus by taking into account its frequency in the document and its frequency in the entire corpus. The TF-IDF score for a word increases proportionally to the number of times it appears in the document but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words are generally more common than others\n",
    "\n",
    "* **tf.data.Dataset.from_tensor_slices** - The tf.data.Dataset.from_tensor_slices function is a method in TensorFlow used for creating a dataset from a given tensor. This function is commonly used for processing data in machine learning models, particularly for input data that can be represented as a tensor. The from_tensor_slices function creates a dataset where each element corresponds to a slice of the input tensor along the first dimension\n",
    "\n",
    "* **tf.keras.preprocessing.image_dataset_from_directory** - a function in TensorFlow that allows you to create a dataset of images from a directory. This function automatically labels the images based on their subdirectory names and returns a TensorFlow Dataset object that can be used for training, evaluation, or prediction. This function also provides options for data augmentation, resizing, and shuffling\n",
    "\n",
    "* **tf.keras.layers.Embedding** - a layer in TensorFlow that is used to create word embeddings in a neural network. The Embedding layer maps each word in the vocabulary to a high-dimensional vector, which is learned during the training process. The Embedding layer is commonly used in natural language processing tasks, such as text classification, sentiment analysis, and machine translation\n",
    "\n",
    "* **tf.keras.Model** - a class in TensorFlow that allows you to create a more complex model by defining the input and output layers and connecting them with any number of layers in between. This class is used for more advanced models that have multiple inputs or outputs or contain shared layers\n",
    "\n",
    "* **tf.keras.Model.predict** - a function in TensorFlow that is used to make predictions using a trained model. The predict function takes a set of input data as input and returns a set of predicted output values. The input data can be in the form of a numpy array or a TensorFlow Dataset object\n",
    "\n",
    "* **tf.keras.preprocessing.image.ImageDataGenerator** - `tf.keras.preprocessing.image.ImageDataGenerator` is a tool in TensorFlow used for data augmentation of images. It can generate new images by applying transformations such as rotation, zooming, and flipping\n",
    "\n",
    "* **tf.keras.preprocessing.sequence.pad_sequences** - a function in TensorFlow that is used to pad sequences of variable length with zeros or truncate them to a fixed length. The function is commonly used in natural language processing to ensure that all sequences have the same length, which is required for input to many machine learning algorithms\n",
    "\n",
    "* **tf.keras.preprocessing.text.one_hot** - a function in TensorFlow that is used to convert a sequence of text into a sequence of one-hot encoded vectors. The function takes the text as input and returns a list of integers, where each integer represents a unique word in the vocabulary. The integers can then be converted to one-hot encoded vectors using the `to_categorical` function in TensorFlow\n",
    "\n",
    "* **time range** - In the context of machine learning and AI, time range refers to the period of time over which data is collected or analyzed. For example, if you are analyzing stock market data, the time range might be a specific year or range of years. The time range can be an important factor in machine learning, as it can affect the type and amount of data available, as well as the modeling techniques that are most appropriate. For example, if you are analyzing time series data, you might use techniques such as autoregression or recurrent neural networks to model the data over time\n",
    "\n",
    "* **tokens** - in natural language processing, a token refers to a sequence of characters that represents a single unit of meaning. Tokens can be words, phrases, or sentences, depending on the level of granularity required for the analysis\n",
    "\n",
    "* **tokenization** - a process of breaking down a text into tokens or units of meaning. Tokenization is a common pre-processing step in natural language processing, which involves splitting the text into words, phrases, or sentences, depending on the requirements of the analysis\n",
    "\n",
    "* **VGG16** - It is a deep convolutional neural network architecture that was developed by the Visual Geometry Group at the University of Oxford. The VGG16 architecture consists of 16 layers, including 13 convolutional layers and 3 fully connected layers. It has been trained on the ImageNet dataset and is widely used for image classification and object detection tasks. The VGG16 architecture has been shown to perform well on a wide range of computer vision tasks and is a popular choice for transfer learning\n",
    "\n",
    "* **visualizing the data** - Visualizing the data involves representing the dataset in a graphical format, such as scatter plots, histograms, and correlation matrices. This helps in identifying trends, patterns, and relationships that are not easily visible in the raw data\n",
    "\n",
    "* **window size** - Window size refers to the number of data points that are included in each batch during training or inference. In time series analysis, window size is used to define the length of the sliding window that is moved along the time axis to create overlapping batches of data. Choosing an appropriate window size can be important in machine learning, as it can affect the model's ability to capture patterns and dependencies in the data. In TensorFlow, window size can be set using functions such as `tf.keras.preprocessing.timeseries_dataset_from_array()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e987d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
